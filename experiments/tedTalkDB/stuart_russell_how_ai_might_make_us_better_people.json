{
  "TalkVideoPage": {
    "VideoURL": "https://www.ted.com/talks/stuart_russell_how_ai_might_make_us_better_people",
    "AvailableSubtitlesCount": "4",
    "Speaker": "Stuart Russell",
    "Duration": "17:35",
    "TimeFilmed": "Apr 2017",
    "TalkViewsCount": "606,422",
    "TalkTopicsList": [
      "AI",
      "Algorithm",
      "Choice",
      "Code",
      "Communication",
      "Computers",
      "Data",
      "Future",
      "Humanity",
      "Innovation",
      "Intelligence",
      "Machine learning",
      "Morality",
      "Programming",
      "Robots",
      "Science",
      "Technology"
    ],
    "TalkCommentsCount": "66"
  },
  "TalkTranscriptPage": {
    "AvailableTranscripts": [
      "English",
      "Hungarian",
      "Persian",
      "Portuguese, Brazilian"
    ],
    "DatePosted": "May 2017",
    "Rated": "Informative, Funny",
    "ImageURL": "https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/3f614c0a546467f66777c847998104c6f185fe99_2880x1620.jpg?cb=20160511\u0026quality=63\u0026u=\u0026w=512",
    "TalkTranscript": {
      "en": {
        "LocalTalkTitle": "3 principles for creating safer AI",
        "Paragraphs": [
          "\nThis is Lee Sedol.\nLee Sedol is one of the world's\ngreatest Go players,\nand he's having what my friends\nin Silicon Valley call\na \"Holy Cow\" moment —\n\n(Laughter)\n\na moment where we realize\nthat AI is actually progressing\na lot faster than we expected.\nSo humans have lost on the Go board.\nWhat about the real world?\n\nWell, the real world is much bigger,\nmuch more complicated than the Go board.\nIt's a lot less visible,\nbut it's still a decision problem.\nAnd if we think about some\nof the technologies\nthat are coming down the pike ...\nNoriko [Arai] mentioned that reading\nis not yet happening in machines,\nat least with understanding.\nBut that will happen,\nand when that happens,\nvery soon afterwards,\nmachines will have read everything\nthat the human race has ever written.\nAnd that will enable machines,\nalong with the ability to look\nfurther ahead than humans can,\nas we've already seen in Go,\nif they also have access\nto more information,\nthey'll be able to make better decisions\nin the real world than we can.\nSo is that a good thing?\nWell, I hope so.\n\nOur entire civilization,\neverything that we value,\nis based on our intelligence.\nAnd if we had access\nto a lot more intelligence,\nthen there's really no limit\nto what the human race can do.\nAnd I think this could be,\nas some people have described it,\nthe biggest event in human history.\nSo why are people saying things like this,\nthat AI might spell the end\nof the human race?\nIs this a new thing?\nIs it just Elon Musk and Bill Gates\nand Stephen Hawking?\n\nActually, no. This idea\nhas been around for a while.\nHere's a quotation:\n\"Even if we could keep the machines\nin a subservient position,\nfor instance, by turning off the power\nat strategic moments\" —\nand I'll come back to that\n\"turning off the power\" idea later on —\n\"we should, as a species,\nfeel greatly humbled.\"\nSo who said this?\nThis is Alan Turing in 1951.\nAlan Turing, as you know,\nis the father of computer science\nand in many ways,\nthe father of AI as well.\nSo if we think about this problem,\nthe problem of creating something\nmore intelligent than your own species,\nwe might call this \"the gorilla problem,\"\nbecause gorillas' ancestors did this\na few million years ago,\nand now we can ask the gorillas:\nWas this a good idea?\n\nSo here they are having a meeting\nto discuss whether it was a good idea,\nand after a little while,\nthey conclude, no,\nthis was a terrible idea.\nOur species is in dire straits.\nIn fact, you can see the existential\nsadness in their eyes.\n\n(Laughter)\n\nSo this queasy feeling that making\nsomething smarter than your own species\nis maybe not a good idea —\nwhat can we do about that?\nWell, really nothing,\nexcept stop doing AI,\nand because of all\nthe benefits that I mentioned\nand because I'm an AI researcher,\nI'm not having that.\nI actually want to be able\nto keep doing AI.\n\nSo we actually need to nail down\nthe problem a bit more.\nWhat exactly is the problem?\nWhy is better AI possibly a catastrophe?\n\nSo here's another quotation:\n\"We had better be quite sure\nthat the purpose put into the machine\nis the purpose which we really desire.\"\nThis was said by Norbert Wiener in 1960,\nshortly after he watched\none of the very early learning systems\nlearn to play checkers\nbetter than its creator.\nBut this could equally have been said\nby King Midas.\nKing Midas said, \"I want everything\nI touch to turn to gold,\"\nand he got exactly what he asked for.\nThat was the purpose\nthat he put into the machine,\nso to speak,\nand then his food and his drink\nand his relatives turned to gold\nand he died in misery and starvation.\nSo we'll call this\n\"the King Midas problem\"\nof stating an objective\nwhich is not, in fact,\ntruly aligned with what we want.\nIn modern terms, we call this\n\"the value alignment problem.\"\n\nPutting in the wrong objective\nis not the only part of the problem.\nThere's another part.\nIf you put an objective into a machine,\neven something as simple as,\n\"Fetch the coffee,\"\nthe machine says to itself,\n\"Well, how might I fail\nto fetch the coffee?\nSomeone might switch me off.\nOK, I have to take steps to prevent that.\nI will disable my 'off' switch.\nI will do anything to defend myself\nagainst interference\nwith this objective\nthat I have been given.\"\nSo this single-minded pursuit\nin a very defensive mode\nof an objective that is, in fact,\nnot aligned with the true objectives\nof the human race —\nthat's the problem that we face.\nAnd in fact, that's the high-value\ntakeaway from this talk.\nIf you want to remember one thing,\nit's that you can't fetch\nthe coffee if you're dead.\n\n(Laughter)\n\nIt's very simple. Just remember that.\nRepeat it to yourself three times a day.\n\n(Laughter)\n\nAnd in fact, this is exactly the plot\nof \"2001: [A Space Odyssey]\"\nHAL has an objective, a mission,\nwhich is not aligned\nwith the objectives of the humans,\nand that leads to this conflict.\nNow fortunately, HAL\nis not superintelligent.\nHe's pretty smart,\nbut eventually Dave outwits him\nand manages to switch him off.\nBut we might not be so lucky.\nSo what are we going to do?\n\nI'm trying to redefine AI\nto get away from this classical notion\nof machines that intelligently\npursue objectives.\nThere are three principles involved.\nThe first one is a principle\nof altruism, if you like,\nthat the robot's only objective\nis to maximize the realization\nof human objectives,\nof human values.\nAnd by values here I don't mean\ntouchy-feely, goody-goody values.\nI just mean whatever it is\nthat the human would prefer\ntheir life to be like.\nAnd so this actually violates Asimov's law\nthat the robot has to protect\nits own existence.\nIt has no interest in preserving\nits existence whatsoever.\n\nThe second law is a law\nof humility, if you like.\nAnd this turns out to be really\nimportant to make robots safe.\nIt says that the robot does not know\nwhat those human values are,\nso it has to maximize them,\nbut it doesn't know what they are.\nAnd that avoids this problem\nof single-minded pursuit\nof an objective.\nThis uncertainty turns out to be crucial.\n\nNow, in order to be useful to us,\nit has to have some idea of what we want.\nIt obtains that information primarily\nby observation of human choices,\nso our own choices reveal information\nabout what it is that we prefer\nour lives to be like.\nSo those are the three principles.\nLet's see how that applies\nto this question of:\n\"Can you switch the machine off?\"\nas Turing suggested.\n\nSo here's a PR2 robot.\nThis is one that we have in our lab,\nand it has a big red \"off\" switch\nright on the back.\nThe question is: Is it\ngoing to let you switch it off?\nIf we do it the classical way,\nwe give it the objective of, \"Fetch\nthe coffee, I must fetch the coffee,\nI can't fetch the coffee if I'm dead,\"\nso obviously the PR2\nhas been listening to my talk,\nand so it says, therefore,\n\"I must disable my 'off' switch,\nand probably taser all the other\npeople in Starbucks\nwho might interfere with me.\"\n\n(Laughter)\n\nSo this seems to be inevitable, right?\nThis kind of failure mode\nseems to be inevitable,\nand it follows from having\na concrete, definite objective.\n\nSo what happens if the machine\nis uncertain about the objective?\nWell, it reasons in a different way.\nIt says, \"OK, the human\nmight switch me off,\nbut only if I'm doing something wrong.\nWell, I don't really know what wrong is,\nbut I know that I don't want to do it.\"\nSo that's the first and second\nprinciples right there.\n\"So I should let the human switch me off.\"\nAnd in fact you can calculate\nthe incentive that the robot has\nto allow the human to switch it off,\nand it's directly tied to the degree\nof uncertainty about\nthe underlying objective.\n\nAnd then when the machine is switched off,\nthat third principle comes into play.\nIt learns something about the objectives\nit should be pursuing,\nbecause it learns that\nwhat it did wasn't right.\nIn fact, we can, with suitable use\nof Greek symbols,\nas mathematicians usually do,\nwe can actually prove a theorem\nthat says that such a robot\nis provably beneficial to the human.\nYou are provably better off\nwith a machine that's designed in this way\nthan without it.\nSo this is a very simple example,\nbut this is the first step\nin what we're trying to do\nwith human-compatible AI.\n\nNow, this third principle,\nI think is the one that you're probably\nscratching your head over.\nYou're probably thinking, \"Well,\nyou know, I behave badly.\nI don't want my robot to behave like me.\nI sneak down in the middle of the night\nand take stuff from the fridge.\nI do this and that.\"\nThere's all kinds of things\nyou don't want the robot doing.\nBut in fact, it doesn't\nquite work that way.\nJust because you behave badly\ndoesn't mean the robot\nis going to copy your behavior.\nIt's going to understand your motivations\nand maybe help you resist them,\nif appropriate.\nBut it's still difficult.\nWhat we're trying to do, in fact,\nis to allow machines to predict\nfor any person and for any possible life\nthat they could live,\nand the lives of everybody else:\nWhich would they prefer?\nAnd there are many, many\ndifficulties involved in doing this;\nI don't expect that this\nis going to get solved very quickly.\nThe real difficulties, in fact, are us.\n\nAs I have already mentioned,\nwe behave badly.\nIn fact, some of us are downright nasty.\nNow the robot, as I said,\ndoesn't have to copy the behavior.\nThe robot does not have\nany objective of its own.\nIt's purely altruistic.\nAnd it's not designed just to satisfy\nthe desires of one person, the user,\nbut in fact it has to respect\nthe preferences of everybody.\nSo it can deal with a certain\namount of nastiness,\nand it can even understand\nthat your nastiness, for example,\nyou may take bribes as a passport official\nbecause you need to feed your family\nand send your kids to school.\nIt can understand that;\nit doesn't mean it's going to steal.\nIn fact, it'll just help you\nsend your kids to school.\n\nWe are also computationally limited.\nLee Sedol is a brilliant Go player,\nbut he still lost.\nSo if we look at his actions,\nhe took an action that lost the game.\nThat doesn't mean he wanted to lose.\nSo to understand his behavior,\nwe actually have to invert\nthrough a model of human cognition\nthat includes our computational\nlimitations — a very complicated model.\nBut it's still something\nthat we can work on understanding.\n\nProbably the most difficult part,\nfrom my point of view as an AI researcher,\nis the fact that there are lots of us,\nand so the machine has to somehow\ntrade off, weigh up the preferences\nof many different people,\nand there are different ways to do that.\nEconomists, sociologists,\nmoral philosophers have understood that,\nand we are actively\nlooking for collaboration.\n\nLet's have a look and see what happens\nwhen you get that wrong.\nSo you can have\na conversation, for example,\nwith your intelligent personal assistant\nthat might be available\nin a few years' time.\nThink of a Siri on steroids.\nSo Siri says, \"Your wife called\nto remind you about dinner tonight.\"\nAnd of course, you've forgotten.\n\"What? What dinner?\nWhat are you talking about?\"\n\n\"Uh, your 20th anniversary at 7pm.\"\n\n\"I can't do that. I'm meeting\nwith the secretary-general at 7:30.\nHow could this have happened?\"\n\n\"Well, I did warn you, but you overrode\nmy recommendation.\"\n\n\"Well, what am I going to do?\nI can't just tell him I'm too busy.\"\n\n\"Don't worry. I arranged\nfor his plane to be delayed.\"\n\n(Laughter)\n\n\"Some kind of computer malfunction.\"\n\n(Laughter)\n\n\"Really? You can do that?\"\n\n\"He sends his profound apologies\nand looks forward to meeting you\nfor lunch tomorrow.\"\n\n(Laughter)\n\nSo the values here —\nthere's a slight mistake going on.\nThis is clearly following my wife's values\nwhich is \"Happy wife, happy life.\"\n\n(Laughter)\n\nIt could go the other way.\nYou could come home\nafter a hard day's work,\nand the computer says, \"Long day?\"\n\n\"Yes, I didn't even have time for lunch.\"\n\n\"You must be very hungry.\"\n\n\"Starving, yeah.\nCould you make some dinner?\"\n\n\"There's something I need to tell you.\"\n\n(Laughter)\n\n\"There are humans in South Sudan\nwho are in more urgent need than you.\"\n\n(Laughter)\n\n\"So I'm leaving. Make your own dinner.\"\n\n(Laughter)\n\nSo we have to solve these problems,\nand I'm looking forward\nto working on them.\n\nThere are reasons for optimism.\nOne reason is,\nthere is a massive amount of data.\nBecause remember — I said\nthey're going to read everything\nthe human race has ever written.\nMost of what we write about\nis human beings doing things\nand other people getting upset about it.\nSo there's a massive amount\nof data to learn from.\n\nThere's also a very\nstrong economic incentive\nto get this right.\nSo imagine your domestic robot's at home.\nYou're late from work again\nand the robot has to feed the kids,\nand the kids are hungry\nand there's nothing in the fridge.\nAnd the robot sees the cat.\n\n(Laughter)\n\nAnd the robot hasn't quite learned\nthe human value function properly,\nso it doesn't understand\nthe sentimental value of the cat outweighs\nthe nutritional value of the cat.\n\n(Laughter)\n\nSo then what happens?\nWell, it happens like this:\n\"Deranged robot cooks kitty\nfor family dinner.\"\nThat one incident would be the end\nof the domestic robot industry.\nSo there's a huge incentive\nto get this right\nlong before we reach\nsuperintelligent machines.\n\nSo to summarize:\nI'm actually trying to change\nthe definition of AI\nso that we have provably\nbeneficial machines.\nAnd the principles are:\nmachines that are altruistic,\nthat want to achieve only our objectives,\nbut that are uncertain\nabout what those objectives are,\nand will watch all of us\nto learn more about what it is\nthat we really want.\nAnd hopefully in the process,\nwe will learn to be better people.\nThank you very much.\n\n(Applause)\n\nChris Anderson: So interesting, Stuart.\nWe're going to stand here a bit\nbecause I think they're setting up\nfor our next speaker.\n\nA couple of questions.\nSo the idea of programming in ignorance\nseems intuitively really powerful.\nAs you get to superintelligence,\nwhat's going to stop a robot\nreading literature and discovering\nthis idea that knowledge\nis actually better than ignorance\nand still just shifting its own goals\nand rewriting that programming?\n\nStuart Russell: Yes, so we want\nit to learn more, as I said,\nabout our objectives.\nIt'll only become more certain\nas it becomes more correct,\nso the evidence is there\nand it's going to be designed\nto interpret it correctly.\nIt will understand, for example,\nthat books are very biased\nin the evidence they contain.\nThey only talk about kings and princes\nand elite white male people doing stuff.\nSo it's a complicated problem,\nbut as it learns more about our objectives\nit will become more and more useful to us.\n\nCA: And you couldn't\njust boil it down to one law,\nyou know, hardwired in:\n\"if any human ever tries to switch me off,\nI comply. I comply.\"\n\nSR: Absolutely not.\nThat would be a terrible idea.\nSo imagine that you have\na self-driving car\nand you want to send your five-year-old\noff to preschool.\nDo you want your five-year-old\nto be able to switch off the car\nwhile it's driving along?\nProbably not.\nSo it needs to understand how rational\nand sensible the person is.\nThe more rational the person,\nthe more willing you are\nto be switched off.\nIf the person is completely\nrandom or even malicious,\nthen you're less willing\nto be switched off.\n\nCA: All right. Stuart, can I just say,\nI really, really hope you\nfigure this out for us.\nThank you so much for that talk.\nThat was amazing.\n\nSR: Thank you.\n\n(Applause)\n"
        ],
        "TimeStamps": [
          "0:11",
          "0:21",
          "0:22",
          "0:32",
          "1:25",
          "2:00",
          "2:48",
          "3:03",
          "3:05",
          "3:29",
          "3:38",
          "4:36",
          "5:27",
          "5:28",
          "5:32",
          "5:34",
          "6:11",
          "6:56",
          "7:20",
          "7:48",
          "8:18",
          "8:20",
          "8:29",
          "9:04",
          "9:41",
          "10:43",
          "11:27",
          "11:56",
          "12:19",
          "12:41",
          "12:47",
          "12:53",
          "12:59",
          "13:03",
          "13:06",
          "13:09",
          "13:11",
          "13:12",
          "13:15",
          "13:20",
          "13:21",
          "13:31",
          "13:32",
          "13:39",
          "13:41",
          "13:42",
          "13:47",
          "13:49",
          "13:51",
          "13:56",
          "13:57",
          "13:59",
          "14:01",
          "14:06",
          "14:22",
          "14:38",
          "14:39",
          "14:50",
          "14:51",
          "15:11",
          "15:38",
          "15:41",
          "15:48",
          "16:08",
          "16:45",
          "16:54",
          "17:23",
          "17:29",
          "17:31"
        ],
        "TalkTranscriptAndTimeStamps": [
          "\n\n 0:11\n\n\nThis is Lee Sedol.\nLee Sedol is one of the world's\ngreatest Go players,\nand he's having what my friends\nin Silicon Valley call\na \"Holy Cow\" moment —\n\n\n\n 0:21\n\n\n(Laughter)\n\n\n\n 0:22\n\n\na moment where we realize\nthat AI is actually progressing\na lot faster than we expected.\nSo humans have lost on the Go board.\nWhat about the real world?\n\n\n\n 0:32\n\n\nWell, the real world is much bigger,\nmuch more complicated than the Go board.\nIt's a lot less visible,\nbut it's still a decision problem.\nAnd if we think about some\nof the technologies\nthat are coming down the pike ...\nNoriko [Arai] mentioned that reading\nis not yet happening in machines,\nat least with understanding.\nBut that will happen,\nand when that happens,\nvery soon afterwards,\nmachines will have read everything\nthat the human race has ever written.\nAnd that will enable machines,\nalong with the ability to look\nfurther ahead than humans can,\nas we've already seen in Go,\nif they also have access\nto more information,\nthey'll be able to make better decisions\nin the real world than we can.\nSo is that a good thing?\nWell, I hope so.\n\n\n\n 1:25\n\n\nOur entire civilization,\neverything that we value,\nis based on our intelligence.\nAnd if we had access\nto a lot more intelligence,\nthen there's really no limit\nto what the human race can do.\nAnd I think this could be,\nas some people have described it,\nthe biggest event in human history.\nSo why are people saying things like this,\nthat AI might spell the end\nof the human race?\nIs this a new thing?\nIs it just Elon Musk and Bill Gates\nand Stephen Hawking?\n\n\n\n 2:00\n\n\nActually, no. This idea\nhas been around for a while.\nHere's a quotation:\n\"Even if we could keep the machines\nin a subservient position,\nfor instance, by turning off the power\nat strategic moments\" —\nand I'll come back to that\n\"turning off the power\" idea later on —\n\"we should, as a species,\nfeel greatly humbled.\"\nSo who said this?\nThis is Alan Turing in 1951.\nAlan Turing, as you know,\nis the father of computer science\nand in many ways,\nthe father of AI as well.\nSo if we think about this problem,\nthe problem of creating something\nmore intelligent than your own species,\nwe might call this \"the gorilla problem,\"\nbecause gorillas' ancestors did this\na few million years ago,\nand now we can ask the gorillas:\nWas this a good idea?\n\n\n\n 2:48\n\n\nSo here they are having a meeting\nto discuss whether it was a good idea,\nand after a little while,\nthey conclude, no,\nthis was a terrible idea.\nOur species is in dire straits.\nIn fact, you can see the existential\nsadness in their eyes.\n\n\n\n 3:03\n\n\n(Laughter)\n\n\n\n 3:05\n\n\nSo this queasy feeling that making\nsomething smarter than your own species\nis maybe not a good idea —\nwhat can we do about that?\nWell, really nothing,\nexcept stop doing AI,\nand because of all\nthe benefits that I mentioned\nand because I'm an AI researcher,\nI'm not having that.\nI actually want to be able\nto keep doing AI.\n\n\n\n 3:29\n\n\nSo we actually need to nail down\nthe problem a bit more.\nWhat exactly is the problem?\nWhy is better AI possibly a catastrophe?\n\n\n\n 3:38\n\n\nSo here's another quotation:\n\"We had better be quite sure\nthat the purpose put into the machine\nis the purpose which we really desire.\"\nThis was said by Norbert Wiener in 1960,\nshortly after he watched\none of the very early learning systems\nlearn to play checkers\nbetter than its creator.\nBut this could equally have been said\nby King Midas.\nKing Midas said, \"I want everything\nI touch to turn to gold,\"\nand he got exactly what he asked for.\nThat was the purpose\nthat he put into the machine,\nso to speak,\nand then his food and his drink\nand his relatives turned to gold\nand he died in misery and starvation.\nSo we'll call this\n\"the King Midas problem\"\nof stating an objective\nwhich is not, in fact,\ntruly aligned with what we want.\nIn modern terms, we call this\n\"the value alignment problem.\"\n\n\n\n 4:36\n\n\nPutting in the wrong objective\nis not the only part of the problem.\nThere's another part.\nIf you put an objective into a machine,\neven something as simple as,\n\"Fetch the coffee,\"\nthe machine says to itself,\n\"Well, how might I fail\nto fetch the coffee?\nSomeone might switch me off.\nOK, I have to take steps to prevent that.\nI will disable my 'off' switch.\nI will do anything to defend myself\nagainst interference\nwith this objective\nthat I have been given.\"\nSo this single-minded pursuit\nin a very defensive mode\nof an objective that is, in fact,\nnot aligned with the true objectives\nof the human race —\nthat's the problem that we face.\nAnd in fact, that's the high-value\ntakeaway from this talk.\nIf you want to remember one thing,\nit's that you can't fetch\nthe coffee if you're dead.\n\n\n\n 5:27\n\n\n(Laughter)\n\n\n\n 5:28\n\n\nIt's very simple. Just remember that.\nRepeat it to yourself three times a day.\n\n\n\n 5:32\n\n\n(Laughter)\n\n\n\n 5:34\n\n\nAnd in fact, this is exactly the plot\nof \"2001: [A Space Odyssey]\"\nHAL has an objective, a mission,\nwhich is not aligned\nwith the objectives of the humans,\nand that leads to this conflict.\nNow fortunately, HAL\nis not superintelligent.\nHe's pretty smart,\nbut eventually Dave outwits him\nand manages to switch him off.\nBut we might not be so lucky.\nSo what are we going to do?\n\n\n\n 6:11\n\n\nI'm trying to redefine AI\nto get away from this classical notion\nof machines that intelligently\npursue objectives.\nThere are three principles involved.\nThe first one is a principle\nof altruism, if you like,\nthat the robot's only objective\nis to maximize the realization\nof human objectives,\nof human values.\nAnd by values here I don't mean\ntouchy-feely, goody-goody values.\nI just mean whatever it is\nthat the human would prefer\ntheir life to be like.\nAnd so this actually violates Asimov's law\nthat the robot has to protect\nits own existence.\nIt has no interest in preserving\nits existence whatsoever.\n\n\n\n 6:56\n\n\nThe second law is a law\nof humility, if you like.\nAnd this turns out to be really\nimportant to make robots safe.\nIt says that the robot does not know\nwhat those human values are,\nso it has to maximize them,\nbut it doesn't know what they are.\nAnd that avoids this problem\nof single-minded pursuit\nof an objective.\nThis uncertainty turns out to be crucial.\n\n\n\n 7:20\n\n\nNow, in order to be useful to us,\nit has to have some idea of what we want.\nIt obtains that information primarily\nby observation of human choices,\nso our own choices reveal information\nabout what it is that we prefer\nour lives to be like.\nSo those are the three principles.\nLet's see how that applies\nto this question of:\n\"Can you switch the machine off?\"\nas Turing suggested.\n\n\n\n 7:48\n\n\nSo here's a PR2 robot.\nThis is one that we have in our lab,\nand it has a big red \"off\" switch\nright on the back.\nThe question is: Is it\ngoing to let you switch it off?\nIf we do it the classical way,\nwe give it the objective of, \"Fetch\nthe coffee, I must fetch the coffee,\nI can't fetch the coffee if I'm dead,\"\nso obviously the PR2\nhas been listening to my talk,\nand so it says, therefore,\n\"I must disable my 'off' switch,\nand probably taser all the other\npeople in Starbucks\nwho might interfere with me.\"\n\n\n\n 8:18\n\n\n(Laughter)\n\n\n\n 8:20\n\n\nSo this seems to be inevitable, right?\nThis kind of failure mode\nseems to be inevitable,\nand it follows from having\na concrete, definite objective.\n\n\n\n 8:29\n\n\nSo what happens if the machine\nis uncertain about the objective?\nWell, it reasons in a different way.\nIt says, \"OK, the human\nmight switch me off,\nbut only if I'm doing something wrong.\nWell, I don't really know what wrong is,\nbut I know that I don't want to do it.\"\nSo that's the first and second\nprinciples right there.\n\"So I should let the human switch me off.\"\nAnd in fact you can calculate\nthe incentive that the robot has\nto allow the human to switch it off,\nand it's directly tied to the degree\nof uncertainty about\nthe underlying objective.\n\n\n\n 9:04\n\n\nAnd then when the machine is switched off,\nthat third principle comes into play.\nIt learns something about the objectives\nit should be pursuing,\nbecause it learns that\nwhat it did wasn't right.\nIn fact, we can, with suitable use\nof Greek symbols,\nas mathematicians usually do,\nwe can actually prove a theorem\nthat says that such a robot\nis provably beneficial to the human.\nYou are provably better off\nwith a machine that's designed in this way\nthan without it.\nSo this is a very simple example,\nbut this is the first step\nin what we're trying to do\nwith human-compatible AI.\n\n\n\n 9:41\n\n\nNow, this third principle,\nI think is the one that you're probably\nscratching your head over.\nYou're probably thinking, \"Well,\nyou know, I behave badly.\nI don't want my robot to behave like me.\nI sneak down in the middle of the night\nand take stuff from the fridge.\nI do this and that.\"\nThere's all kinds of things\nyou don't want the robot doing.\nBut in fact, it doesn't\nquite work that way.\nJust because you behave badly\ndoesn't mean the robot\nis going to copy your behavior.\nIt's going to understand your motivations\nand maybe help you resist them,\nif appropriate.\nBut it's still difficult.\nWhat we're trying to do, in fact,\nis to allow machines to predict\nfor any person and for any possible life\nthat they could live,\nand the lives of everybody else:\nWhich would they prefer?\nAnd there are many, many\ndifficulties involved in doing this;\nI don't expect that this\nis going to get solved very quickly.\nThe real difficulties, in fact, are us.\n\n\n\n10:43\n\n\nAs I have already mentioned,\nwe behave badly.\nIn fact, some of us are downright nasty.\nNow the robot, as I said,\ndoesn't have to copy the behavior.\nThe robot does not have\nany objective of its own.\nIt's purely altruistic.\nAnd it's not designed just to satisfy\nthe desires of one person, the user,\nbut in fact it has to respect\nthe preferences of everybody.\nSo it can deal with a certain\namount of nastiness,\nand it can even understand\nthat your nastiness, for example,\nyou may take bribes as a passport official\nbecause you need to feed your family\nand send your kids to school.\nIt can understand that;\nit doesn't mean it's going to steal.\nIn fact, it'll just help you\nsend your kids to school.\n\n\n\n11:27\n\n\nWe are also computationally limited.\nLee Sedol is a brilliant Go player,\nbut he still lost.\nSo if we look at his actions,\nhe took an action that lost the game.\nThat doesn't mean he wanted to lose.\nSo to understand his behavior,\nwe actually have to invert\nthrough a model of human cognition\nthat includes our computational\nlimitations — a very complicated model.\nBut it's still something\nthat we can work on understanding.\n\n\n\n11:56\n\n\nProbably the most difficult part,\nfrom my point of view as an AI researcher,\nis the fact that there are lots of us,\nand so the machine has to somehow\ntrade off, weigh up the preferences\nof many different people,\nand there are different ways to do that.\nEconomists, sociologists,\nmoral philosophers have understood that,\nand we are actively\nlooking for collaboration.\n\n\n\n12:19\n\n\nLet's have a look and see what happens\nwhen you get that wrong.\nSo you can have\na conversation, for example,\nwith your intelligent personal assistant\nthat might be available\nin a few years' time.\nThink of a Siri on steroids.\nSo Siri says, \"Your wife called\nto remind you about dinner tonight.\"\nAnd of course, you've forgotten.\n\"What? What dinner?\nWhat are you talking about?\"\n\n\n\n12:41\n\n\n\"Uh, your 20th anniversary at 7pm.\"\n\n\n\n12:47\n\n\n\"I can't do that. I'm meeting\nwith the secretary-general at 7:30.\nHow could this have happened?\"\n\n\n\n12:53\n\n\n\"Well, I did warn you, but you overrode\nmy recommendation.\"\n\n\n\n12:59\n\n\n\"Well, what am I going to do?\nI can't just tell him I'm too busy.\"\n\n\n\n13:03\n\n\n\"Don't worry. I arranged\nfor his plane to be delayed.\"\n\n\n\n13:06\n\n\n(Laughter)\n\n\n\n13:09\n\n\n\"Some kind of computer malfunction.\"\n\n\n\n13:11\n\n\n(Laughter)\n\n\n\n13:12\n\n\n\"Really? You can do that?\"\n\n\n\n13:15\n\n\n\"He sends his profound apologies\nand looks forward to meeting you\nfor lunch tomorrow.\"\n\n\n\n13:20\n\n\n(Laughter)\n\n\n\n13:21\n\n\nSo the values here —\nthere's a slight mistake going on.\nThis is clearly following my wife's values\nwhich is \"Happy wife, happy life.\"\n\n\n\n13:31\n\n\n(Laughter)\n\n\n\n13:32\n\n\nIt could go the other way.\nYou could come home\nafter a hard day's work,\nand the computer says, \"Long day?\"\n\n\n\n13:39\n\n\n\"Yes, I didn't even have time for lunch.\"\n\n\n\n13:41\n\n\n\"You must be very hungry.\"\n\n\n\n13:42\n\n\n\"Starving, yeah.\nCould you make some dinner?\"\n\n\n\n13:47\n\n\n\"There's something I need to tell you.\"\n\n\n\n13:49\n\n\n(Laughter)\n\n\n\n13:51\n\n\n\"There are humans in South Sudan\nwho are in more urgent need than you.\"\n\n\n\n13:56\n\n\n(Laughter)\n\n\n\n13:57\n\n\n\"So I'm leaving. Make your own dinner.\"\n\n\n\n13:59\n\n\n(Laughter)\n\n\n\n14:01\n\n\nSo we have to solve these problems,\nand I'm looking forward\nto working on them.\n\n\n\n14:06\n\n\nThere are reasons for optimism.\nOne reason is,\nthere is a massive amount of data.\nBecause remember — I said\nthey're going to read everything\nthe human race has ever written.\nMost of what we write about\nis human beings doing things\nand other people getting upset about it.\nSo there's a massive amount\nof data to learn from.\n\n\n\n14:22\n\n\nThere's also a very\nstrong economic incentive\nto get this right.\nSo imagine your domestic robot's at home.\nYou're late from work again\nand the robot has to feed the kids,\nand the kids are hungry\nand there's nothing in the fridge.\nAnd the robot sees the cat.\n\n\n\n14:38\n\n\n(Laughter)\n\n\n\n14:39\n\n\nAnd the robot hasn't quite learned\nthe human value function properly,\nso it doesn't understand\nthe sentimental value of the cat outweighs\nthe nutritional value of the cat.\n\n\n\n14:50\n\n\n(Laughter)\n\n\n\n14:51\n\n\nSo then what happens?\nWell, it happens like this:\n\"Deranged robot cooks kitty\nfor family dinner.\"\nThat one incident would be the end\nof the domestic robot industry.\nSo there's a huge incentive\nto get this right\nlong before we reach\nsuperintelligent machines.\n\n\n\n15:11\n\n\nSo to summarize:\nI'm actually trying to change\nthe definition of AI\nso that we have provably\nbeneficial machines.\nAnd the principles are:\nmachines that are altruistic,\nthat want to achieve only our objectives,\nbut that are uncertain\nabout what those objectives are,\nand will watch all of us\nto learn more about what it is\nthat we really want.\nAnd hopefully in the process,\nwe will learn to be better people.\nThank you very much.\n\n\n\n15:38\n\n\n(Applause)\n\n\n\n15:41\n\n\nChris Anderson: So interesting, Stuart.\nWe're going to stand here a bit\nbecause I think they're setting up\nfor our next speaker.\n\n\n\n15:48\n\n\nA couple of questions.\nSo the idea of programming in ignorance\nseems intuitively really powerful.\nAs you get to superintelligence,\nwhat's going to stop a robot\nreading literature and discovering\nthis idea that knowledge\nis actually better than ignorance\nand still just shifting its own goals\nand rewriting that programming?\n\n\n\n16:08\n\n\nStuart Russell: Yes, so we want\nit to learn more, as I said,\nabout our objectives.\nIt'll only become more certain\nas it becomes more correct,\nso the evidence is there\nand it's going to be designed\nto interpret it correctly.\nIt will understand, for example,\nthat books are very biased\nin the evidence they contain.\nThey only talk about kings and princes\nand elite white male people doing stuff.\nSo it's a complicated problem,\nbut as it learns more about our objectives\nit will become more and more useful to us.\n\n\n\n16:45\n\n\nCA: And you couldn't\njust boil it down to one law,\nyou know, hardwired in:\n\"if any human ever tries to switch me off,\nI comply. I comply.\"\n\n\n\n16:54\n\n\nSR: Absolutely not.\nThat would be a terrible idea.\nSo imagine that you have\na self-driving car\nand you want to send your five-year-old\noff to preschool.\nDo you want your five-year-old\nto be able to switch off the car\nwhile it's driving along?\nProbably not.\nSo it needs to understand how rational\nand sensible the person is.\nThe more rational the person,\nthe more willing you are\nto be switched off.\nIf the person is completely\nrandom or even malicious,\nthen you're less willing\nto be switched off.\n\n\n\n17:23\n\n\nCA: All right. Stuart, can I just say,\nI really, really hope you\nfigure this out for us.\nThank you so much for that talk.\nThat was amazing.\n\n\n\n17:29\n\n\nSR: Thank you.\n\n\n\n17:31\n\n\n(Applause)\n\n"
        ]
      },
      "fa": {
        "LocalTalkTitle": "چگونه هوش مصنوعی ما را تبدیل به مردمانی بهتر می‌کند",
        "Paragraphs": [
          "\nاین لی سادل هست.\nلی سادل یکی از بزرگترین\nبازیکن های Go (یک بازی فکری)در جهان هست.\nو در حال تجربه لحظه‌ای هست\nکه دوستان من در سیلیکون ولی\nبهش میگن لحظه \"یا پیغمبر!\"\n\n(خنده حاضرین)\n\nلحظه‌ای که م‌ف‌همیم\nهوش مصنوعی واقعا داره سریعتر از چیزی که\nانتظارشو داشتیم پیشرفت میکنه.\nپس، انسان‌ها روی تخته Go باخته‌‌اند.\nدر دنیای واقعی چطور؟\n\nباید بگم که دنیای واقعی خیلی بزرگتر\nو بسیار بسیار پیچیده‌تر\nاز تخته بازی Go هست.\n(دنیای واقعی) خیلی نامرئی‌تر هست،\nولی همچنان مساله تصمیم گیری هست.\nو اگر درباره برخی از تکنولوژی‌ها فکر کنیم\nکه در حال ظهور هستند...\n[Noriko [Arai اشاره کرده است که\nتوانایی خواندن هنوز در ماشین‌ها وجود ندارد،\nحداقل همراه با فهمیدن نیست.\nولی این اتفاق خواهد افتاد.\nو وقتی که به وقوع بپیوندد،\nپس از آن، خیلی زود\nماشین‌ها تمام آن‌چه را که\nبشر نوشته است، خواهند خواند.\nو این ماشین‌ها را قادر می‌سازد\nکه فراتر از انسان‌ها به آینده نگاه کنند،\nهمانطور که قبلاً در ‌Go دیده‌ایم،\nاگر ماشین‌ها به اطلاعات بیشتری\nدسترسی داشته باشند،\nمی‌توانند تصمیمات بهتری\nدر جهان واقعی نسبت به ما بگیرند.\nآیا این یک اتفاق خوب است؟\nخب، امیدوارم که باشه.\n\nتمام تمدن ما،\nهر چیزی که برایش ارزش قائل هستیم،\nبر پایه هوشمندی ما است.\nو اگر ما هوش بیشتری در اختیار داشتیم،\nآن‌ وقت هیچ حد و مرزی برای کارهایی\nکه انسان می‌تواند بکند وجود نداشت.\nو من فکر می‌کنم که،\nهمانطور که برخی توصیف کرد‌اند،\nاین می‌تواند\nبزرگترین رویداد تاریخ بشریت باشد.\nپس چرا بعضی‌ها حرفهایی میزنند،\nمثلا اینکه هوش مصنوعی می‌تواند\nخاتمه دهنده نسل بشر باشد؟\nآیا این یک پدیده جدید است؟\nآیا فقط ایلان ماسک و بیل گیتس\nو استیون هاوکینگ هستند؟\n\nراستش، نه.\nاین ایده خیلی وقته که وجود دارد.\nیه نقل قول میگه:\n«حتی اگر میتونستیم ماشین‌ها رو\nفرمانبردار نگه داریم\nمثلا با خاموش کردنشان در لحظات استراتژیک»\n— و من بعداً\nبه ایده «خاموش کردن»برمی‌گردم—\n«ما به عنوان یک گونه،\nباید خیلی احساس پستی کنیم»\nکی این رو گفته؟\nآلن تورینگ در سال ۱۹۵۱\nآلن تورینگ، همانطور که می‌دانید\nپدر علم کامپیوتر هست.\nو از خیلی از جهات،\nپدر علم هوش مصنوعی هم هست.\nپس اگر درباره این مساله فکر کنیم،\nمساله ساختن چیزی هوشمندتر از\nگونه خودمان،\nشاید این رو «مساله گوریل» بنامیم.\nچون اجداد گوریل‌ها این کار رو\nچند میلیون‌ سال قبل انجام داده اند،\nو الان می‌توانیم از گوریل‌ها بپرسیم که:\nآیا این کار ایده‌ خوبی بود؟\n\nاینم از گوریل‌هایی که در جلسه‌ای،\nدرباره اینکه آیا ایده خوبی بود بحث میکنند\nو بعد از مدت کوتاهی،\nبه این نتیجه میرسن که، نه\nیک ایده افتضاح بود\nگونه ما،‌ در تنگنای بدی قرار دارد\nدر واقع، شما می‌توانید غم عالم رو\nدر چشمانشان ببینید\n\n(خنده حاضرین)\n\nپس شاید این احساس ناراحتی از به وجود آوردن\nچیزی هوشمندتر از گونه خود\nایده خوبی نباشد\nما چه کاری از دستمان برمی‌آید؟\nدرواقع، هیچی به جز متوقف کردن هوش مصنوعی،\nو به دلیل تمام فوایدی که گفتم\nو به دلیل اینکه من یک \nمحقق هوش مصنوعی هستم\nمن این مورد رو قبول ندارم.\nمن میخوام که بتوانم همچنان\nروی هوش مصنوعی کار کنم.\n\nپس باید این مساله رو بیشتر واکاوی کنیم.\nمشکل واقعا چی هست؟\nچرا هوش مصنوعی بهتر\nمنجر به فاجعه می‌شود؟\n\nاینم یک نقل قول دیگه:\n«بهتره که مطمئن باشیم\nهدفی که در ماشین‌ قرار‌ میدهیم\nهمان هدفی است که واقعا میخواهیم.»\nکه توسط نوربرت وینر در ۱۹۶۰ گفته شده،\nبلافاصله بعد از اینکه وی دید\nیکی از سیستم‌های یادگیرنده اولیه\nبازی چکرز رو\nبهتر از سازندگانش بازی می‌کند.\nولی این جمله می‌توانست توسط\nشاه میداس هم گفته شده باشد.\nشاه میداس گفت: «من میخواهم\nهرچه را که لمس میکنم تبدیل به طلا شود،»\nو او دقیقاً چیزی را که خواسته بود گرفت.\nو آن هدفی بود که وی در ماشین قرار داد.\nاینطور که میگن،\nو بعدش غذا، نوشیدنی و اقوامش\nتبدیل به طلا شدند.\nو از بدبختی و گشنگی مرد.\nپس ما به این مشکل میگوییم:\n«مشکل شاه میداس»\nکه در آن هدف را چیزی قرار می‌دهیم،\nکه واقعاً هم جهت با\nچیزی که ما مي‌خواهیم نیست.\nبه بیان جدیدتر، به این مشکل میگیم:\n«مشکل هم جهت سازی ارزش»\n\nهدف گذاری اشتباه تنها بخش مشکل نیست.\nبخش دیگری هم هست\nاگر شما یک هدف برای ماشین قرار دهید\nحتی به سادگیج «آوردن قهوه»\nماشین به خودش میگه\n«چطوری ممکنه که من\nنتونم قهوه رو بیارم؟\nیکی ممکنه منو خاموش کنه..\nخب پس من باید کاری کنم\nکه جلوی این کار رو بگیرم.\nمن دکمه «خاموش» خودمو غیرفعال می‌کنم.\nمن هرکاری میکنم تا\nاز خودم در برابر موانعی که\nسد راه هدفی که به من داده شده می‌شوند،\nدفاع کنم.»\nبنابراین، این دنبال کردن تک-ذهنه\nدر یک حالت خیلی دفاعی از هدف،\nدر واقع، هم جهت با اهداف گونه انسان نیست.\nاین مشکلی هست \nکه باهاش مواجه هستیم\nو در واقع، این\nنکته با ارزش این سخنرانی هست.\nاگر میخواهید \nفقط یک چیز را به خاطرتون بسپرید،\nاون اینه که اگر شما بمیرین،\nدیگه نمی‌تونین قهوه رو بیارین\n\n(خنده حاضرین)\n\nخیلی سادست. فقط اینو یادتون باشه.\nروزی سه بار با خودتون تکرار کنین.\n\n(خنده حضار)\n\nو در واقع، این نکته\nدقیقا پلات فیلم 2001 [A space Odyssey] است.\nHAL یک هدف داره، یک ماموریت\n\nکه هم‌جهت با اهداف انسان‌ها نیست\nو این باعث بروز مشکلاتی میشه\nالبته خوشبختانه، HAL خیلی باهوش نیست\nاون نسبتا باهوش هست، ولی در نهایت\nDave گولش میزنه.\nو میتونه خاموشش کنه.\nولی شاید ما خیلی خوش‌ شانس نباشیم.\nپس باید چیکار کنیم؟\n\nمن سعی میکنم هوش مصنوعی رو باز تعریف کنم.\nتا از این تصور سنتی بیرون بیایم.\nکه طبق اون، ماشین‌هایی هستند \nکه به صورت هوشمند اهداف رو دنبال میکنن.\n۳ تا اصل رو باید در نظر گرفت.\nاولین اصل، نوع دوستی هست.\nاگر شما دوست داشته باشین\nتنها هدف ربات\nاین باشه که اهداف انسان رو واقعیت ببخشه.\nو ارزش‌های انسانی رو.\nو منظورم از ارزش‌ها،\nارزش‌های احساسی یا خیرخواهانه نیست.\n\nمنظورم هر چیزی هست که انسان ترجیح میده\nزندگیش اون شکلی باشه.\nو این در واقع قانون آسیموف رو نقض میکنه.\nکه ربات باید از حیات خودش محافظت کنه.\nربات هیچ علاقه‌ای\nبه مراقبت از حیات خودش نداره.\n\nقانون دوم، قانون فروتنی هست.\nالبته اگه خوشتون بیاد.\nو این قانون نقش مهمی\nدر امن کردن ربات‌ها داره.\nطبق این قانون، ربات نمیدونه که\nارزش‌های انسانی چه چیز‌هایی هستند\nباید در راستای محقق شدنشون تلاش کنه،\nولی نمیدونه چه چیزهایی هستند.\nو این از مشکل دنبال کردن تک-ذهنی هدف\n\nجلوگیری می‌کنه.\nاین عدم قطعیت بسیار مهم هست.\n\nحالا، برای اینکه ربات برای ما مفید باشه\nباید یک ایده‌ای\nازچیزی که میخوایم داشته باشه.\nو این اطلاعات رو در درجه اول\nاز مشاهده انتخاب‌های انسان به دست می‌آره.\nپس، انتخاب‌های خود ما هستند\nکه اطلاعات رو آشکار میکنن\nدرباره چیزی که ما ترجیح میدیم\nزندگیمون شبیه به اون باشه.\nپس این ۳ اصل بیان شد.\nحالا بیاین ببینیم که این اصول،\nچگونه روی این سوال عمل میکند:\n«آیا میتونی ماشین رو خاموش کنی؟»\nهمانطور که تورینگ پیشنهاد داد.\n\nاین یک ربات مدل PR2 هستش.\nکه ما یک نمونه از آن در آزمایشگاهمون داریم\nو یک دکمه بزرگ قرمز\nبرای «خاموش» کردن در پشتش داره.\nسوال اینه که آیا\nاین ربات بهتون اجازه میده که خاموشش کنین؟\nاگر ما از راه سنتی عمل کنیم،\nبهش هدف این هدف رو میدیم:\n«قهوه روبیار، من باید قهوه رو بیارم\nمن نمیتونم قهوه بیارم اگر مرده باشم»\nبه وضوح، PR2 به سخنرانی من گوش کرده،\nپس میگه\nمن باید دکمه «خاموش» رو غیرفعال کنم.\nو احتمالاً با دستگاه شوک،\nبه تمام مردم داخل استارباکس شلیک کنم!\nکسانی که ممکنه سد راه من باشن.\n\n(خنده حاضرین)\n\nپس این مساله\nبه نظر اجتناب ناپذیر میاد، درسته؟\nاین حالت شکست به نظر اجتناب ناپذیر هست،\nو از داشتن یک هدف دقیق و محکم نشأت میگیره.\n\nحالا چه اتفاقی می‌افته\nاگه ماشین درباره هدف مطمئن نباشه؟\nاینطوری، یک جور دیگه استدلال میکنه.\nمیگه: «باشه، انسان ممکنه منو خاموش کنه\nولی تنها در صورتی این کارو میکنه\nکه من کار اشتباهی بکنم.\nمن نمیدونم چه کاری اشتباهه\nولی میدونم که نمیخوام انجامش بدم.»\nاین اصل اول و دوم گفته شده بود.\n«پس باید بذارم که که انسان منو خاموش کنه.»\nو درواقع، شما میتونین انگیزه ای که ربات\nبرای خاموش کردنش\nتوسط انسان دارد رو محاسبه کنید،\nو این مستقیماً مرتبطه با درجه عدم قطعیت\nدرباره اهداف اصولی دارد.\n\nو وقتی که ماشین خاموش بشه\nاصل سوم وارد عمل میشه.\nربات یه چیزی درباره اهداف یاد میگیره،\nباید پیگیر باشه،\nچون‌ یاد میگیره کاری که کرده درست نبوده.\nدر واقع، ما میتونیم\nبا استفاده درست از نماد‌های یونانی\nهمانطور که معمولاً\nریاضیدانان این کار رو میکنن\nمیتونیم یک قضیه رو ثابت کنیم\nکه میگه،‌ این چنین رباتی\nقطعا برای انسان مفید است.\nقطعا وجود ماشینی که اینطوری طراحی شده باشه\n\nاز نبودنش بهتره.\nاین یک مثال ساده است،\nولی قدم اول راه ماست.\nراه استفاده از هوش مصنوعی سازگار با انسان.\n\nحالا، اصل سوم\nفکر کنم چیزی باشه\nکه احتمالا درکش براتون سخت باشه.\nاحتمالا شما فکر میکنین که\n«راستش، من بد رفتار میکنم\nو نمیخوام که ربات من مثل من رفتار کنه.\nمن نصف شب دزدکی میرم سر یخچال.\nیا فلان کار رو میکنم.»\nخیلی از کارها هست که شما دوست ندارین\nرباتتون انجامشون بده\nولی در واقع، اصل مطلب اینطوری نیست.\nبه صرف اینکه شما بد رفتار میکنین،\nدلیل نمیشه ربات هم از رفتار شما تقلید کنه.\nربات قراره که انگیزه‌های شما رو بفهمه،\nو شاید در راستای رسیدن بهش کمکتون کنه،\nالبته اگر مناسب باشه.\nولی همچنان کاری سختیه.\nدر واقع، کاری که ما سعی میکنیم انجام بدیم\nاینه که به ماشین‌ها اجازه بدیم\nبرای هر فرد و هر زندگی ممکن پیش‌بینی کنند\nکه آیا میتونن زنده بمونن\nو البته جان بقیه:\nاین که کدوم رو ترجیح میدن؟\nو سختی‌های بسیار زیادی\nبرای انجام این کار وجود دارن؛\nو من انتظار ندارم\nکه این مساله به زودی‌‌ حل بشه.\nو چالش اصلی، در واقع خود ما هستیم.\n\nهمانطور که قبلا اشاره کردم،\nما بد رفتار میکنیم.\nدر واقعا بعضی از ما\nکاملا بدجنس هستیم.\nحالا همانطور که گفتم،\nربات مجبور نیست که رفتار رو تقلید کنه.\nربات از خودش هیچ هدفی ندارد.\nو کاملا نوع دوست هست.\nو برای این طراحی نشده که خواسته های\nیک انسان یا کاربر رو برآورده کنه،\nدر حقیقت،‌ ربات باید\nبه ترجیحات همه احترام بگذارد.\nپس میتونه مقدار مشخصی\nاز بدرفتاری رو تحمل کنه،\nو حتی میتونه سوءرفتار شما رو درک کنه،\nمثلا اینکه شما\nبه عنوان مامور اداره گذرنامه رشوه میگیرد\nدلیلش اینه که شما نان آور خانواده اید\nو بچه هاتون رو به مدرسه بفرستین.\nربات میتونه این موضوع رو بفهمه،\nو اینطوری نیست که دزدی کنه.\nدر واقع کمکتون خواهد کرد\nکه بچه‌هاتون رو به مدرسه بفرستین.\n\nما همچنین از نظر محاسباتی هم محدود هستیم.\nLee Sedol یک بازیکن با استعداد Go هست.\nولی با این وجود بازنده است.\nپس اگر به حرکاتش دقت کنیم،\nیک حرکتی بود که به شکستش منجر شد.\nاین به این معنی نیست که اون میخواست ببازه.\nپس برای فهمیدن رفتارش\nما باید از یک مدل\nانسان شناختی استفاده کنیم\nکه شامل محدودیت‌های محاسباتی می‌شود.\nکه یک مدل بسیار پیچیده است.\nولی همچنان این چیزی هست\nکه برای فهمیدن میتونیم روش کار کنیم.\n\nبه نظر من به عنوان یک محقق هوش مصنوعی،\nدشوارترین بخش ماجرا\nاین حقیقته که \nتعداد زیادی از ما(انسان‌ها) وجود دارد.\nبنابراین ماشین‌ها باید به طریقی\nیک مصالحه‌ و توازن\nبین ترجیحات افراد مختلف برقرار کنن.\nو راه‌های زیادی برای این کار وجود دارند.\nاقتصاددان‌ها، جامعه شناس ها،\nفلاسفه اخلاق مدار متوجه شده اند\nکه ما فعالانه در حال همکاری هستیم.\n\nبیاین ببینیم وقتی که\nاشتباه برداشت کنید چه اتفاقی خواهد افتاد.\nشما میتونین یک گفتگو داشته باشین،\nمثلا با دستیار شخصی هوشمندتان،\nکه احتمالا در آینده نزدیک در دسترس باشد.\nمثلا به یک نمونه قوی از Siri فکر کنید\nSiri بهتون میگه که: «همسرتون زنگ زد\nتا برای شام امشب بهتون یادآوری کنه»\nو البته شما هم فراموش کرده بودید\nچی؟ کدوم شام؟\nراجع به چی حرف میزنی؟\n\nامم...بیستمین سالگرد ازدواجتون ساعت ۷ شب\n\nمن نمیتونم برم.\nساعت ۷:۳۰ با دبیرکل سازمان ملل جلسه دارم.\nچطوری اینطوری شد؟»\n\n«من بهت هشدار داده بودم،\nولی تو پیشنهاد من رو نشنیده گرفتی.»\n\n«خب حالا چیکار کنم؟\nنمیتونم بهش بگم که من خیلی سرم شلوغه.\n\nنگران نباش. من پروازش رو طوری گذاشتم\nکه تاخیر داشته باشه.»\n\n(خنده حاضرین)\n\n«یک نوع خطای کارکرد کامپیوتری.»\n\n(خنده حاضرین)\n\n«جدی؟ تو میتونی این کارو بکنی؟»\n\n«ایشان ازتون عمیقاً عذرخواهی خواهد کرد.\nو از شما خواهد خواست که\nفردا ناهار همدیگررو ببینین.»\n\n(خنده حاضرین)\n\nولی اینجا یک مشکلی هست.\nربات به وضوح داره\nبه ارزش‌های همسر من توجه میکنه:\nکه همون \"همسر شاد، زندگی شاد\" است.\n\n(خنده حاضرین)\n\nمیتونست جور دیگه ای باشه.\nممکن بود شما بعد از یک روز سخت کاری\nبیاین خونه،\nو کامپیوتر بگه:\n«روز سخت و طولانی بود؟»\n\n«آره، حتی برای ناهار خوردن هم وقت نداشتم.»\n\n«باید خیلی گرسنت باشه.»\n\n«آره دارم از گشنگی میمیرم.\nمیتونی برام شام درست کنی؟»\n\n«یه چیزی هست که باید بهت بگم.»\n\n(خنده حاضرین)\n\n«یه سری آدم در سودان جنوبی هستن\nکه بیشتر از تو در شرایط اضطراری قرار دارن»\n\n(خنده حاضرین)\n\n«پس من میرم. خودت شامت رو درست کن.»\n\n(خنده حاضرین)\n\nبنابراین باید این مشکلات رو حل کنیم\nو من با رغبت دارم\nروی این مشکلات کار میکنم.\n\nدلایلی برای خوشبین بودن وجود دارن.\nیک دلیل،\nاینه که حجم زیادی از داده داریم.\nچون اگر یادتون باشه،\nمن گفتم که ربات‌ها هر چیزی که\nانسان تا کنون نوشته\nرا خواهند خواند.\nبیشتر چیزهایی که ما مینویسیم\nدرباره کارهای بشر هستش.\nو بقیه مردم به خاطرش ناراحت میشن.\nپس انبوهی از داده برای یادگیری وجود داره.\n\nیک انگیزه اقتصادی قوی هم برای انجام\nاین کار هست.\nربات خدمتکارتون در منزل رو تصور کنید.\nشما باز هم دیر از سر کار بر میگردین،\nو ربات باید به بچه‌ها غذا بده.\nبچه‌ها گشنه هستند\nو هیچ غذایی در یخچال نیست.\nو ربات گربه رو میبینه!\n\n(خنده حاضرین)\n\nو ربات تابع ارزش گذاری انسان ها رو\nبه خوبی یاد نگرفته\nبنابراین نمیفهمه\nکه ارزش معنوی گربه\nاز ارزش غذایی آن بیشتر است.\n\n(خنده حضار)\n\nبعدش چی میشه؟\nیه چیزی تو این مایه‌ها:\n\"ربات ظالم، برای شام خانواده گربه می‌پزد\"\nاین حادثه احتمالا\nپایان صنعت ربات‌های خانگی باشد.\nپس انگیزه زیادی\nبرای درست شدن این موضوع وجود دارد.\nخیلی زودتر از اینکه\nبه ماشین‌های فوق هوشمند برسیم.\n\nپس برای جمع بندی:\nمن در اصل دارم سعی میکنم\nکه تعریف هوش مصنوعی رو طوری عوض کنم\nکه ماشین های سودمندی داشته باشیم.\nو این اصول عبارتند از:\nماشین‌ها نوع دوست هستند.\nو فقط میخوان به اهدافی که ما داریم برسن.\nولی درباره چیستی این اهداف مطمئن نیستند.\nو به ما نگاه میکنند\nتا یادبگیرند\nاین اهدافی که ما میخواهیم چه هستند.\nو امیدوارم در این فرآیند،\nما هم یادبگیریم که مردم بهتری باشیم.\nخیلی ممنون.\n\n(تشویق حاضرین)\n\nکریس اندسون: خیلی جالب بود استوارت.\nما یکم باید اینجا وایستیم\nچون عوامل دارن صحنه رو\nبرای سخنرانی بعدی آماده میکنن.\n\nچندتا سوال:\nایده برنامه‌ریزی در بی خبری \nبه نظر خیلی قدرتمند میاد.\nوقتی که که به هوش قوی برسیم.\nچه چیزی مانع رباتی میشه که\nمتنی رو میخونه و به این ایده میرسه که\nاون دانش بهتر از ندانستن است\nو اهدافش رو تغییر بده\nو دوباره برنامه ریزی کنه؟\n\nاستوارت راسل: آره همانطور که گفتم،\nما میخوایم که ربات درباره اهداف ما\nبیشتر یاد بگیره.\nربات فقط وقتی مطمئن تر میشه\nکه موضوع درست‌تر باشه.\nشواهد گویا هستند.\nو ربات طوری طراحی میشه که درست تفسیر کنه.\nمثلا میفهمه که کتاب‌ها خیلی\nدر شواهدی که دارند\n\nجانبدارانه عمل میکنند.\nفقط درباره پادشاهان و شاهزادگان حرف میزنن.\nو مردان سفید پوست فوق‌ العاده ای\nکه مشغول کاری هستند.\nبنابراین این یک مشکل پیچیده است.\n\nولی همانطور که ربات\nداره درباره اهداف ما یاد میگیره.\nبرای ما بیشتر و بیشتر کاربردی میشه.\n\nک آ: پس نمیشه اینو تبدیل به یک قانون کنیم\nو به صورت کورکورانه بگیم که:\n«اگر هر انسانی سعی کرد منو خاموش کنه\nمن پیروی میکنم. من پیروی میکنم.»\n\nقطعا اینطور نیست.\nاینطوری خیلی بد میشد.\nمثلا تصور کنید که یک ماشین خودران دارید.\nو میخواهید بچه پنج سالتون رو\nبه مدرسه بفرستید.\nآیا شما میخواین که بچه پنج سالتون\nبتونه ماشین رو موقع رانندگی\nخاموش کنه؟\nاحتمالا نه.\nپس ربات باید بتونه درک کنه\nکه آدم چقدر منطقی و معقول هست.\nهرچه انسان منطقی تر باشد،\nربات بیشتر تمایل دارد\nکه بگذارد خاموش شود.\nاگر شخص کاملا تصادفی یا خرابکار باشد\nربات کمتر تمایل دارد که بگذارد خاموش شود.\n\nخب استوارت، فقط میتونم بگم که\nمن واقعا، واقعا امیدوارم تو برای ما\nاین مشکل رو حل کنی.\nممنون بابت سخنرانی.\nفوق العاده بود.\n\nممنون.\n\n(تشویق حضار)\n"
        ],
        "TimeStamps": [
          "0:11",
          "0:21",
          "0:22",
          "0:32",
          "1:25",
          "2:00",
          "2:48",
          "3:03",
          "3:05",
          "3:29",
          "3:38",
          "4:36",
          "5:27",
          "5:28",
          "5:32",
          "5:34",
          "6:11",
          "6:56",
          "7:20",
          "7:48",
          "8:18",
          "8:20",
          "8:29",
          "9:04",
          "9:41",
          "10:43",
          "11:27",
          "11:56",
          "12:19",
          "12:41",
          "12:47",
          "12:53",
          "12:59",
          "13:03",
          "13:06",
          "13:09",
          "13:11",
          "13:12",
          "13:15",
          "13:20",
          "13:21",
          "13:31",
          "13:32",
          "13:39",
          "13:41",
          "13:42",
          "13:47",
          "13:49",
          "13:51",
          "13:56",
          "13:57",
          "13:59",
          "14:01",
          "14:06",
          "14:22",
          "14:38",
          "14:39",
          "14:50",
          "14:51",
          "15:11",
          "15:38",
          "15:41",
          "15:48",
          "16:08",
          "16:45",
          "16:54",
          "17:23",
          "17:29",
          "17:31"
        ],
        "TalkTranscriptAndTimeStamps": [
          "\n\n 0:11\n\n\nاین لی سادل هست.\nلی سادل یکی از بزرگترین\nبازیکن های Go (یک بازی فکری)در جهان هست.\nو در حال تجربه لحظه‌ای هست\nکه دوستان من در سیلیکون ولی\nبهش میگن لحظه \"یا پیغمبر!\"\n\n\n\n 0:21\n\n\n(خنده حاضرین)\n\n\n\n 0:22\n\n\nلحظه‌ای که م‌ف‌همیم\nهوش مصنوعی واقعا داره سریعتر از چیزی که\nانتظارشو داشتیم پیشرفت میکنه.\nپس، انسان‌ها روی تخته Go باخته‌‌اند.\nدر دنیای واقعی چطور؟\n\n\n\n 0:32\n\n\nباید بگم که دنیای واقعی خیلی بزرگتر\nو بسیار بسیار پیچیده‌تر\nاز تخته بازی Go هست.\n(دنیای واقعی) خیلی نامرئی‌تر هست،\nولی همچنان مساله تصمیم گیری هست.\nو اگر درباره برخی از تکنولوژی‌ها فکر کنیم\nکه در حال ظهور هستند...\n[Noriko [Arai اشاره کرده است که\nتوانایی خواندن هنوز در ماشین‌ها وجود ندارد،\nحداقل همراه با فهمیدن نیست.\nولی این اتفاق خواهد افتاد.\nو وقتی که به وقوع بپیوندد،\nپس از آن، خیلی زود\nماشین‌ها تمام آن‌چه را که\nبشر نوشته است، خواهند خواند.\nو این ماشین‌ها را قادر می‌سازد\nکه فراتر از انسان‌ها به آینده نگاه کنند،\nهمانطور که قبلاً در ‌Go دیده‌ایم،\nاگر ماشین‌ها به اطلاعات بیشتری\nدسترسی داشته باشند،\nمی‌توانند تصمیمات بهتری\nدر جهان واقعی نسبت به ما بگیرند.\nآیا این یک اتفاق خوب است؟\nخب، امیدوارم که باشه.\n\n\n\n 1:25\n\n\nتمام تمدن ما،\nهر چیزی که برایش ارزش قائل هستیم،\nبر پایه هوشمندی ما است.\nو اگر ما هوش بیشتری در اختیار داشتیم،\nآن‌ وقت هیچ حد و مرزی برای کارهایی\nکه انسان می‌تواند بکند وجود نداشت.\nو من فکر می‌کنم که،\nهمانطور که برخی توصیف کرد‌اند،\nاین می‌تواند\nبزرگترین رویداد تاریخ بشریت باشد.\nپس چرا بعضی‌ها حرفهایی میزنند،\nمثلا اینکه هوش مصنوعی می‌تواند\nخاتمه دهنده نسل بشر باشد؟\nآیا این یک پدیده جدید است؟\nآیا فقط ایلان ماسک و بیل گیتس\nو استیون هاوکینگ هستند؟\n\n\n\n 2:00\n\n\nراستش، نه.\nاین ایده خیلی وقته که وجود دارد.\nیه نقل قول میگه:\n«حتی اگر میتونستیم ماشین‌ها رو\nفرمانبردار نگه داریم\nمثلا با خاموش کردنشان در لحظات استراتژیک»\n— و من بعداً\nبه ایده «خاموش کردن»برمی‌گردم—\n«ما به عنوان یک گونه،\nباید خیلی احساس پستی کنیم»\nکی این رو گفته؟\nآلن تورینگ در سال ۱۹۵۱\nآلن تورینگ، همانطور که می‌دانید\nپدر علم کامپیوتر هست.\nو از خیلی از جهات،\nپدر علم هوش مصنوعی هم هست.\nپس اگر درباره این مساله فکر کنیم،\nمساله ساختن چیزی هوشمندتر از\nگونه خودمان،\nشاید این رو «مساله گوریل» بنامیم.\nچون اجداد گوریل‌ها این کار رو\nچند میلیون‌ سال قبل انجام داده اند،\nو الان می‌توانیم از گوریل‌ها بپرسیم که:\nآیا این کار ایده‌ خوبی بود؟\n\n\n\n 2:48\n\n\nاینم از گوریل‌هایی که در جلسه‌ای،\nدرباره اینکه آیا ایده خوبی بود بحث میکنند\nو بعد از مدت کوتاهی،\nبه این نتیجه میرسن که، نه\nیک ایده افتضاح بود\nگونه ما،‌ در تنگنای بدی قرار دارد\nدر واقع، شما می‌توانید غم عالم رو\nدر چشمانشان ببینید\n\n\n\n 3:03\n\n\n(خنده حاضرین)\n\n\n\n 3:05\n\n\nپس شاید این احساس ناراحتی از به وجود آوردن\nچیزی هوشمندتر از گونه خود\nایده خوبی نباشد\nما چه کاری از دستمان برمی‌آید؟\nدرواقع، هیچی به جز متوقف کردن هوش مصنوعی،\nو به دلیل تمام فوایدی که گفتم\nو به دلیل اینکه من یک \nمحقق هوش مصنوعی هستم\nمن این مورد رو قبول ندارم.\nمن میخوام که بتوانم همچنان\nروی هوش مصنوعی کار کنم.\n\n\n\n 3:29\n\n\nپس باید این مساله رو بیشتر واکاوی کنیم.\nمشکل واقعا چی هست؟\nچرا هوش مصنوعی بهتر\nمنجر به فاجعه می‌شود؟\n\n\n\n 3:38\n\n\nاینم یک نقل قول دیگه:\n«بهتره که مطمئن باشیم\nهدفی که در ماشین‌ قرار‌ میدهیم\nهمان هدفی است که واقعا میخواهیم.»\nکه توسط نوربرت وینر در ۱۹۶۰ گفته شده،\nبلافاصله بعد از اینکه وی دید\nیکی از سیستم‌های یادگیرنده اولیه\nبازی چکرز رو\nبهتر از سازندگانش بازی می‌کند.\nولی این جمله می‌توانست توسط\nشاه میداس هم گفته شده باشد.\nشاه میداس گفت: «من میخواهم\nهرچه را که لمس میکنم تبدیل به طلا شود،»\nو او دقیقاً چیزی را که خواسته بود گرفت.\nو آن هدفی بود که وی در ماشین قرار داد.\nاینطور که میگن،\nو بعدش غذا، نوشیدنی و اقوامش\nتبدیل به طلا شدند.\nو از بدبختی و گشنگی مرد.\nپس ما به این مشکل میگوییم:\n«مشکل شاه میداس»\nکه در آن هدف را چیزی قرار می‌دهیم،\nکه واقعاً هم جهت با\nچیزی که ما مي‌خواهیم نیست.\nبه بیان جدیدتر، به این مشکل میگیم:\n«مشکل هم جهت سازی ارزش»\n\n\n\n 4:36\n\n\nهدف گذاری اشتباه تنها بخش مشکل نیست.\nبخش دیگری هم هست\nاگر شما یک هدف برای ماشین قرار دهید\nحتی به سادگیج «آوردن قهوه»\nماشین به خودش میگه\n«چطوری ممکنه که من\nنتونم قهوه رو بیارم؟\nیکی ممکنه منو خاموش کنه..\nخب پس من باید کاری کنم\nکه جلوی این کار رو بگیرم.\nمن دکمه «خاموش» خودمو غیرفعال می‌کنم.\nمن هرکاری میکنم تا\nاز خودم در برابر موانعی که\nسد راه هدفی که به من داده شده می‌شوند،\nدفاع کنم.»\nبنابراین، این دنبال کردن تک-ذهنه\nدر یک حالت خیلی دفاعی از هدف،\nدر واقع، هم جهت با اهداف گونه انسان نیست.\nاین مشکلی هست \nکه باهاش مواجه هستیم\nو در واقع، این\nنکته با ارزش این سخنرانی هست.\nاگر میخواهید \nفقط یک چیز را به خاطرتون بسپرید،\nاون اینه که اگر شما بمیرین،\nدیگه نمی‌تونین قهوه رو بیارین\n\n\n\n 5:27\n\n\n(خنده حاضرین)\n\n\n\n 5:28\n\n\nخیلی سادست. فقط اینو یادتون باشه.\nروزی سه بار با خودتون تکرار کنین.\n\n\n\n 5:32\n\n\n(خنده حضار)\n\n\n\n 5:34\n\n\nو در واقع، این نکته\nدقیقا پلات فیلم 2001 [A space Odyssey] است.\nHAL یک هدف داره، یک ماموریت\n\nکه هم‌جهت با اهداف انسان‌ها نیست\nو این باعث بروز مشکلاتی میشه\nالبته خوشبختانه، HAL خیلی باهوش نیست\nاون نسبتا باهوش هست، ولی در نهایت\nDave گولش میزنه.\nو میتونه خاموشش کنه.\nولی شاید ما خیلی خوش‌ شانس نباشیم.\nپس باید چیکار کنیم؟\n\n\n\n 6:11\n\n\nمن سعی میکنم هوش مصنوعی رو باز تعریف کنم.\nتا از این تصور سنتی بیرون بیایم.\nکه طبق اون، ماشین‌هایی هستند \nکه به صورت هوشمند اهداف رو دنبال میکنن.\n۳ تا اصل رو باید در نظر گرفت.\nاولین اصل، نوع دوستی هست.\nاگر شما دوست داشته باشین\nتنها هدف ربات\nاین باشه که اهداف انسان رو واقعیت ببخشه.\nو ارزش‌های انسانی رو.\nو منظورم از ارزش‌ها،\nارزش‌های احساسی یا خیرخواهانه نیست.\n\nمنظورم هر چیزی هست که انسان ترجیح میده\nزندگیش اون شکلی باشه.\nو این در واقع قانون آسیموف رو نقض میکنه.\nکه ربات باید از حیات خودش محافظت کنه.\nربات هیچ علاقه‌ای\nبه مراقبت از حیات خودش نداره.\n\n\n\n 6:56\n\n\nقانون دوم، قانون فروتنی هست.\nالبته اگه خوشتون بیاد.\nو این قانون نقش مهمی\nدر امن کردن ربات‌ها داره.\nطبق این قانون، ربات نمیدونه که\nارزش‌های انسانی چه چیز‌هایی هستند\nباید در راستای محقق شدنشون تلاش کنه،\nولی نمیدونه چه چیزهایی هستند.\nو این از مشکل دنبال کردن تک-ذهنی هدف\n\nجلوگیری می‌کنه.\nاین عدم قطعیت بسیار مهم هست.\n\n\n\n 7:20\n\n\nحالا، برای اینکه ربات برای ما مفید باشه\nباید یک ایده‌ای\nازچیزی که میخوایم داشته باشه.\nو این اطلاعات رو در درجه اول\nاز مشاهده انتخاب‌های انسان به دست می‌آره.\nپس، انتخاب‌های خود ما هستند\nکه اطلاعات رو آشکار میکنن\nدرباره چیزی که ما ترجیح میدیم\nزندگیمون شبیه به اون باشه.\nپس این ۳ اصل بیان شد.\nحالا بیاین ببینیم که این اصول،\nچگونه روی این سوال عمل میکند:\n«آیا میتونی ماشین رو خاموش کنی؟»\nهمانطور که تورینگ پیشنهاد داد.\n\n\n\n 7:48\n\n\nاین یک ربات مدل PR2 هستش.\nکه ما یک نمونه از آن در آزمایشگاهمون داریم\nو یک دکمه بزرگ قرمز\nبرای «خاموش» کردن در پشتش داره.\nسوال اینه که آیا\nاین ربات بهتون اجازه میده که خاموشش کنین؟\nاگر ما از راه سنتی عمل کنیم،\nبهش هدف این هدف رو میدیم:\n«قهوه روبیار، من باید قهوه رو بیارم\nمن نمیتونم قهوه بیارم اگر مرده باشم»\nبه وضوح، PR2 به سخنرانی من گوش کرده،\nپس میگه\nمن باید دکمه «خاموش» رو غیرفعال کنم.\nو احتمالاً با دستگاه شوک،\nبه تمام مردم داخل استارباکس شلیک کنم!\nکسانی که ممکنه سد راه من باشن.\n\n\n\n 8:18\n\n\n(خنده حاضرین)\n\n\n\n 8:20\n\n\nپس این مساله\nبه نظر اجتناب ناپذیر میاد، درسته؟\nاین حالت شکست به نظر اجتناب ناپذیر هست،\nو از داشتن یک هدف دقیق و محکم نشأت میگیره.\n\n\n\n 8:29\n\n\nحالا چه اتفاقی می‌افته\nاگه ماشین درباره هدف مطمئن نباشه؟\nاینطوری، یک جور دیگه استدلال میکنه.\nمیگه: «باشه، انسان ممکنه منو خاموش کنه\nولی تنها در صورتی این کارو میکنه\nکه من کار اشتباهی بکنم.\nمن نمیدونم چه کاری اشتباهه\nولی میدونم که نمیخوام انجامش بدم.»\nاین اصل اول و دوم گفته شده بود.\n«پس باید بذارم که که انسان منو خاموش کنه.»\nو درواقع، شما میتونین انگیزه ای که ربات\nبرای خاموش کردنش\nتوسط انسان دارد رو محاسبه کنید،\nو این مستقیماً مرتبطه با درجه عدم قطعیت\nدرباره اهداف اصولی دارد.\n\n\n\n 9:04\n\n\nو وقتی که ماشین خاموش بشه\nاصل سوم وارد عمل میشه.\nربات یه چیزی درباره اهداف یاد میگیره،\nباید پیگیر باشه،\nچون‌ یاد میگیره کاری که کرده درست نبوده.\nدر واقع، ما میتونیم\nبا استفاده درست از نماد‌های یونانی\nهمانطور که معمولاً\nریاضیدانان این کار رو میکنن\nمیتونیم یک قضیه رو ثابت کنیم\nکه میگه،‌ این چنین رباتی\nقطعا برای انسان مفید است.\nقطعا وجود ماشینی که اینطوری طراحی شده باشه\n\nاز نبودنش بهتره.\nاین یک مثال ساده است،\nولی قدم اول راه ماست.\nراه استفاده از هوش مصنوعی سازگار با انسان.\n\n\n\n 9:41\n\n\nحالا، اصل سوم\nفکر کنم چیزی باشه\nکه احتمالا درکش براتون سخت باشه.\nاحتمالا شما فکر میکنین که\n«راستش، من بد رفتار میکنم\nو نمیخوام که ربات من مثل من رفتار کنه.\nمن نصف شب دزدکی میرم سر یخچال.\nیا فلان کار رو میکنم.»\nخیلی از کارها هست که شما دوست ندارین\nرباتتون انجامشون بده\nولی در واقع، اصل مطلب اینطوری نیست.\nبه صرف اینکه شما بد رفتار میکنین،\nدلیل نمیشه ربات هم از رفتار شما تقلید کنه.\nربات قراره که انگیزه‌های شما رو بفهمه،\nو شاید در راستای رسیدن بهش کمکتون کنه،\nالبته اگر مناسب باشه.\nولی همچنان کاری سختیه.\nدر واقع، کاری که ما سعی میکنیم انجام بدیم\nاینه که به ماشین‌ها اجازه بدیم\nبرای هر فرد و هر زندگی ممکن پیش‌بینی کنند\nکه آیا میتونن زنده بمونن\nو البته جان بقیه:\nاین که کدوم رو ترجیح میدن؟\nو سختی‌های بسیار زیادی\nبرای انجام این کار وجود دارن؛\nو من انتظار ندارم\nکه این مساله به زودی‌‌ حل بشه.\nو چالش اصلی، در واقع خود ما هستیم.\n\n\n\n10:43\n\n\nهمانطور که قبلا اشاره کردم،\nما بد رفتار میکنیم.\nدر واقعا بعضی از ما\nکاملا بدجنس هستیم.\nحالا همانطور که گفتم،\nربات مجبور نیست که رفتار رو تقلید کنه.\nربات از خودش هیچ هدفی ندارد.\nو کاملا نوع دوست هست.\nو برای این طراحی نشده که خواسته های\nیک انسان یا کاربر رو برآورده کنه،\nدر حقیقت،‌ ربات باید\nبه ترجیحات همه احترام بگذارد.\nپس میتونه مقدار مشخصی\nاز بدرفتاری رو تحمل کنه،\nو حتی میتونه سوءرفتار شما رو درک کنه،\nمثلا اینکه شما\nبه عنوان مامور اداره گذرنامه رشوه میگیرد\nدلیلش اینه که شما نان آور خانواده اید\nو بچه هاتون رو به مدرسه بفرستین.\nربات میتونه این موضوع رو بفهمه،\nو اینطوری نیست که دزدی کنه.\nدر واقع کمکتون خواهد کرد\nکه بچه‌هاتون رو به مدرسه بفرستین.\n\n\n\n11:27\n\n\nما همچنین از نظر محاسباتی هم محدود هستیم.\nLee Sedol یک بازیکن با استعداد Go هست.\nولی با این وجود بازنده است.\nپس اگر به حرکاتش دقت کنیم،\nیک حرکتی بود که به شکستش منجر شد.\nاین به این معنی نیست که اون میخواست ببازه.\nپس برای فهمیدن رفتارش\nما باید از یک مدل\nانسان شناختی استفاده کنیم\nکه شامل محدودیت‌های محاسباتی می‌شود.\nکه یک مدل بسیار پیچیده است.\nولی همچنان این چیزی هست\nکه برای فهمیدن میتونیم روش کار کنیم.\n\n\n\n11:56\n\n\nبه نظر من به عنوان یک محقق هوش مصنوعی،\nدشوارترین بخش ماجرا\nاین حقیقته که \nتعداد زیادی از ما(انسان‌ها) وجود دارد.\nبنابراین ماشین‌ها باید به طریقی\nیک مصالحه‌ و توازن\nبین ترجیحات افراد مختلف برقرار کنن.\nو راه‌های زیادی برای این کار وجود دارند.\nاقتصاددان‌ها، جامعه شناس ها،\nفلاسفه اخلاق مدار متوجه شده اند\nکه ما فعالانه در حال همکاری هستیم.\n\n\n\n12:19\n\n\nبیاین ببینیم وقتی که\nاشتباه برداشت کنید چه اتفاقی خواهد افتاد.\nشما میتونین یک گفتگو داشته باشین،\nمثلا با دستیار شخصی هوشمندتان،\nکه احتمالا در آینده نزدیک در دسترس باشد.\nمثلا به یک نمونه قوی از Siri فکر کنید\nSiri بهتون میگه که: «همسرتون زنگ زد\nتا برای شام امشب بهتون یادآوری کنه»\nو البته شما هم فراموش کرده بودید\nچی؟ کدوم شام؟\nراجع به چی حرف میزنی؟\n\n\n\n12:41\n\n\nامم...بیستمین سالگرد ازدواجتون ساعت ۷ شب\n\n\n\n12:47\n\n\nمن نمیتونم برم.\nساعت ۷:۳۰ با دبیرکل سازمان ملل جلسه دارم.\nچطوری اینطوری شد؟»\n\n\n\n12:53\n\n\n«من بهت هشدار داده بودم،\nولی تو پیشنهاد من رو نشنیده گرفتی.»\n\n\n\n12:59\n\n\n«خب حالا چیکار کنم؟\nنمیتونم بهش بگم که من خیلی سرم شلوغه.\n\n\n\n13:03\n\n\nنگران نباش. من پروازش رو طوری گذاشتم\nکه تاخیر داشته باشه.»\n\n\n\n13:06\n\n\n(خنده حاضرین)\n\n\n\n13:09\n\n\n«یک نوع خطای کارکرد کامپیوتری.»\n\n\n\n13:11\n\n\n(خنده حاضرین)\n\n\n\n13:12\n\n\n«جدی؟ تو میتونی این کارو بکنی؟»\n\n\n\n13:15\n\n\n«ایشان ازتون عمیقاً عذرخواهی خواهد کرد.\nو از شما خواهد خواست که\nفردا ناهار همدیگررو ببینین.»\n\n\n\n13:20\n\n\n(خنده حاضرین)\n\n\n\n13:21\n\n\nولی اینجا یک مشکلی هست.\nربات به وضوح داره\nبه ارزش‌های همسر من توجه میکنه:\nکه همون \"همسر شاد، زندگی شاد\" است.\n\n\n\n13:31\n\n\n(خنده حاضرین)\n\n\n\n13:32\n\n\nمیتونست جور دیگه ای باشه.\nممکن بود شما بعد از یک روز سخت کاری\nبیاین خونه،\nو کامپیوتر بگه:\n«روز سخت و طولانی بود؟»\n\n\n\n13:39\n\n\n«آره، حتی برای ناهار خوردن هم وقت نداشتم.»\n\n\n\n13:41\n\n\n«باید خیلی گرسنت باشه.»\n\n\n\n13:42\n\n\n«آره دارم از گشنگی میمیرم.\nمیتونی برام شام درست کنی؟»\n\n\n\n13:47\n\n\n«یه چیزی هست که باید بهت بگم.»\n\n\n\n13:49\n\n\n(خنده حاضرین)\n\n\n\n13:51\n\n\n«یه سری آدم در سودان جنوبی هستن\nکه بیشتر از تو در شرایط اضطراری قرار دارن»\n\n\n\n13:56\n\n\n(خنده حاضرین)\n\n\n\n13:57\n\n\n«پس من میرم. خودت شامت رو درست کن.»\n\n\n\n13:59\n\n\n(خنده حاضرین)\n\n\n\n14:01\n\n\nبنابراین باید این مشکلات رو حل کنیم\nو من با رغبت دارم\nروی این مشکلات کار میکنم.\n\n\n\n14:06\n\n\nدلایلی برای خوشبین بودن وجود دارن.\nیک دلیل،\nاینه که حجم زیادی از داده داریم.\nچون اگر یادتون باشه،\nمن گفتم که ربات‌ها هر چیزی که\nانسان تا کنون نوشته\nرا خواهند خواند.\nبیشتر چیزهایی که ما مینویسیم\nدرباره کارهای بشر هستش.\nو بقیه مردم به خاطرش ناراحت میشن.\nپس انبوهی از داده برای یادگیری وجود داره.\n\n\n\n14:22\n\n\nیک انگیزه اقتصادی قوی هم برای انجام\nاین کار هست.\nربات خدمتکارتون در منزل رو تصور کنید.\nشما باز هم دیر از سر کار بر میگردین،\nو ربات باید به بچه‌ها غذا بده.\nبچه‌ها گشنه هستند\nو هیچ غذایی در یخچال نیست.\nو ربات گربه رو میبینه!\n\n\n\n14:38\n\n\n(خنده حاضرین)\n\n\n\n14:39\n\n\nو ربات تابع ارزش گذاری انسان ها رو\nبه خوبی یاد نگرفته\nبنابراین نمیفهمه\nکه ارزش معنوی گربه\nاز ارزش غذایی آن بیشتر است.\n\n\n\n14:50\n\n\n(خنده حضار)\n\n\n\n14:51\n\n\nبعدش چی میشه؟\nیه چیزی تو این مایه‌ها:\n\"ربات ظالم، برای شام خانواده گربه می‌پزد\"\nاین حادثه احتمالا\nپایان صنعت ربات‌های خانگی باشد.\nپس انگیزه زیادی\nبرای درست شدن این موضوع وجود دارد.\nخیلی زودتر از اینکه\nبه ماشین‌های فوق هوشمند برسیم.\n\n\n\n15:11\n\n\nپس برای جمع بندی:\nمن در اصل دارم سعی میکنم\nکه تعریف هوش مصنوعی رو طوری عوض کنم\nکه ماشین های سودمندی داشته باشیم.\nو این اصول عبارتند از:\nماشین‌ها نوع دوست هستند.\nو فقط میخوان به اهدافی که ما داریم برسن.\nولی درباره چیستی این اهداف مطمئن نیستند.\nو به ما نگاه میکنند\nتا یادبگیرند\nاین اهدافی که ما میخواهیم چه هستند.\nو امیدوارم در این فرآیند،\nما هم یادبگیریم که مردم بهتری باشیم.\nخیلی ممنون.\n\n\n\n15:38\n\n\n(تشویق حاضرین)\n\n\n\n15:41\n\n\nکریس اندسون: خیلی جالب بود استوارت.\nما یکم باید اینجا وایستیم\nچون عوامل دارن صحنه رو\nبرای سخنرانی بعدی آماده میکنن.\n\n\n\n15:48\n\n\nچندتا سوال:\nایده برنامه‌ریزی در بی خبری \nبه نظر خیلی قدرتمند میاد.\nوقتی که که به هوش قوی برسیم.\nچه چیزی مانع رباتی میشه که\nمتنی رو میخونه و به این ایده میرسه که\nاون دانش بهتر از ندانستن است\nو اهدافش رو تغییر بده\nو دوباره برنامه ریزی کنه؟\n\n\n\n16:08\n\n\nاستوارت راسل: آره همانطور که گفتم،\nما میخوایم که ربات درباره اهداف ما\nبیشتر یاد بگیره.\nربات فقط وقتی مطمئن تر میشه\nکه موضوع درست‌تر باشه.\nشواهد گویا هستند.\nو ربات طوری طراحی میشه که درست تفسیر کنه.\nمثلا میفهمه که کتاب‌ها خیلی\nدر شواهدی که دارند\n\nجانبدارانه عمل میکنند.\nفقط درباره پادشاهان و شاهزادگان حرف میزنن.\nو مردان سفید پوست فوق‌ العاده ای\nکه مشغول کاری هستند.\nبنابراین این یک مشکل پیچیده است.\n\nولی همانطور که ربات\nداره درباره اهداف ما یاد میگیره.\nبرای ما بیشتر و بیشتر کاربردی میشه.\n\n\n\n16:45\n\n\nک آ: پس نمیشه اینو تبدیل به یک قانون کنیم\nو به صورت کورکورانه بگیم که:\n«اگر هر انسانی سعی کرد منو خاموش کنه\nمن پیروی میکنم. من پیروی میکنم.»\n\n\n\n16:54\n\n\nقطعا اینطور نیست.\nاینطوری خیلی بد میشد.\nمثلا تصور کنید که یک ماشین خودران دارید.\nو میخواهید بچه پنج سالتون رو\nبه مدرسه بفرستید.\nآیا شما میخواین که بچه پنج سالتون\nبتونه ماشین رو موقع رانندگی\nخاموش کنه؟\nاحتمالا نه.\nپس ربات باید بتونه درک کنه\nکه آدم چقدر منطقی و معقول هست.\nهرچه انسان منطقی تر باشد،\nربات بیشتر تمایل دارد\nکه بگذارد خاموش شود.\nاگر شخص کاملا تصادفی یا خرابکار باشد\nربات کمتر تمایل دارد که بگذارد خاموش شود.\n\n\n\n17:23\n\n\nخب استوارت، فقط میتونم بگم که\nمن واقعا، واقعا امیدوارم تو برای ما\nاین مشکل رو حل کنی.\nممنون بابت سخنرانی.\nفوق العاده بود.\n\n\n\n17:29\n\n\nممنون.\n\n\n\n17:31\n\n\n(تشویق حضار)\n\n"
        ]
      },
      "hu": {
        "LocalTalkTitle": "Miképp tehet jobbá a mesterséges intelligencia?",
        "Paragraphs": [
          "\nŐ Lee Sedol,\na világ egyik legjobb Go-játékosa,\naki itt épp arra gondol, amit\nSzilícium-völgyi barátaim csak úgy hívnak:\n\"azt a leborult szivarvégit!\".\n\n(Nevetés)\n\nA pillanat, amikor rádöbbenünk,\nhogy a MI sokkal gyorsabban fejlődik,\nmint amire számítottunk.\nA Go táblán az ember alul maradt.\nDe mi lesz a való életben?\n\nA való világ sokkal nagyobb,\nsokszorta összetettebb, mint a Go.\nNem annyira látható,\nde az is döntési probléma.\nHa néhány új technológiára gondolunk,\nmelyek szárnyaikat bontogatják...\nNoriko [Arai] megemlítette,\nhogy a gépek még nem olvasnak,\nlegalábbis még nem értik.\nDe ez is el fog jönni,\nés amikor bekövetkezik,\nonnan már nem kell sok,\nhogy elolvassanak mindent,\namit az emberi faj valaha leírt.\nEz képessé fogja tenni őket,\nhogy távolabbra lássanak,\nmint amire az ember képes,\nahogy a Go-ban már láttuk.\nHa még több információhoz is hozzáférnek,\nképesek lesznek a való világban is\njobb döntéseket hozni nálunk.\nVajon ez jó hír?\nRemélem.\n\nA teljes civilizációnk, minden,\namit számunkra értéket képvisel,\naz intelligenciánkon alapul.\nHa sokkal több intelligenciához\nférnénk hozzá,\nhatártalan lehetőségek nyílnának meg\naz emberi faj előtt.\nAzt hiszem, ez lehet -\nahogy néhányan megfogalmazták -\naz emberi történelem legnagyobb eseménye.\nMégis miért hangzanak el akkor olyanok,\nhogy a MI az emberi faj végét jelentheti?\nEz valami új dolog?\nCsak Elon Musk, Bill Gates\nés Stephen Hawking gondolja így?\n\nValójában nem. \nEz már régebb óta kísértő elgondolás.\nNézzük ezt az idézetet:\n\"Még ha képesek is lennénk a gépeket\nalárendelt szolgai szerepben tartani,\npéldául úgy, hogy kritikus helyzetekben\nkikapcsolnánk őket,\" -\nkésőbb még visszatérek\nerre a kikapcsolás-ötletre -\n\"mi, mint élőlények, nagyon megalázva\néreznénk magunkat.\"\nKi mondta ezt?\nAlan Turing 1951-ben.\nAlarn Turingról tudjuk,\nhogy a számítástudomány atyja,\nés sok tekintetben a MI atyja is.\nEzt a problémakört,\namikor saját fajunknál intelligensebb\nteremtményeket hozunk létre,\nhívhatjuk a \"gorilla-problémának\",\nmivel gorilla őseink épp ezt csinálták,\nnéhány millió évvel ezelőtt,\nígy most megkérdezhetjük a gorillákat:\nJó ötlet volt?\n\nItt éppen arról értekeznek,\nhogy jó ötlet volt-e,\nés kisvártatva arra jutnak, hogy nem,\nez szörnyű ötlet volt.\nFajunk a kihalás szélén áll.\nValójában a lét szomorúsága\ntükröződik a szemükben.\n\n(Nevetés)\n\nA saját fajunknál valami\nokosabbat létrehozni,\ntalán nem is jó ötlet,\nami felkavaró érzés -\nmégis hogyan kerülhetjük el?\nTulajdonképpen sehogy,\nkivéve, ha leállítjuk a MI kutatást,\nde a sok előny, amit említettem,\nés mert én MI-kutató vagyok,\nnem állítanám le.\nIgazából folytatni szeretném \na MI fejlesztést.\n\nEgy kissé pontosítanunk kell a problémát.\nMi is igazából a probléma?\nMiért jelenthet katasztrófát egy jobb MI?\n\nVegyünk egy másik idézetet:\n\"Jó lesz nagyon odafigyelnünk rá,\nhogy a gépbe táplált cél\nvalóban az a cél, amit tényleg akarunk.\"\nEzt Norbert Wiener mondta 1960-ban,\nközvetlen azután, hogy látott \negy nagyon korai tanuló rendszert,\namelyik az alkotójánál\njobb volt a dámajátékban.\nDe ugyanezt elmondhatta volna\n\nMidász király is.\nÍgy szólt: \"Azt akarom, változzon \nminden arannyá, amit megérintek\",\nés pontosan azt kapta, amit kért.\nEz volt a cél, amit betáplált a gépbe,\nmondhatjuk így is,\nígy aztán az étele, az itala,\nés a rokonai is arannyá változtak,\nő pedig nyomorúságos éhhalált halt.\nSzóval ezt hívjuk \n\"Midász király problémának\",\namikor olyan célt nevezünk meg,\nami valójában\nnem igazodik jól szándékainkhoz.\nMai kifejezéssel ezt \n\"érték-illesztési problémának\" nevezzük.\n\nA probléma nem csak a rossz cél\nbetáplálását jelenti.\nVan egy másik oldala is.\nHa egy célt előírunk egy gépnek,\nakár olyan egyszerűt is, mint:\n\"Hozd ide a kávét\",\na gép azt mondja magának:\n\"Miképp vallhatok kudarcot,\na kávé oda vitelében?\"\nPéldául valaki közben kikapcsol.\nRendben, akkor ezt meg kell akadályoznom.\nLe fogom tiltani a kikapcsológombomat.\nMindent megteszek\naz akadályok elhárításáért\na cél érdekében, amit feladatul kaptam.\"\nSzóval ez az együgyű törekvés\nilyen önvédelmező módon\nolyan cél felé viszi,\nami igazából nem illeszkedik jól\naz emberi faj valós céljaihoz -\nez a probléma, amivel szembesülünk.\nValójában ez a nagyon értékes\nútravaló üzenete ennek az előadásnak.\nHa csak egy dologra szeretnének emlékezni,\nez legyen: nem tudod felszolgálni a kávét,\nha meghaltál.\n\n(Nevetés)\n\nRoppant egyszerű. Csak erre emlékezzenek!\nIsmételjék el naponta háromszor!\n\n(Nevetés)\n\nTulajdonképpen erről szól\na \"2001: Űrodüsszeia\".\nHAL-nak van egy célja, egy küldetése,\nmely nincs az emberi igényekhez igazítva,\nés ez nézeteltérésekhez vezet.\nDe szerencsére HAL nem szuperintelligens.\nMeglehetősen okos,\nde végül Dave túljár az eszén,\nés sikerül kikapcsolnia.\nDe lehet, hogy mi nem leszünk\nilyen szerencsések.\nAkkor mit tegyünk majd?\n\nMegpróbálom újraértelmezni az MI-t,\nhogy eltávolodjunk ettől\na klasszikus szemlélettől,\nmely szerint az intelligens gépek\ncélokért küzdenek.\nHárom alapelvről van szó.\nAz első az altruizmus alapelve,\nha úgy tetszik:\na robot egyetlen célja,\nhogy végsőkig segítse\naz emberi értékekhez igazodó\nemberi célok megvalósulását.\nÉrtékek alatt nem mézes-mázas,\nszuper jó értékekre gondolok.\nHanem arra, amilyennek az emberek\naz életüket szeretnék.\nEz valójában megsérti Asimov törvényét,\nhogy a robotnak meg kell védenie\nsaját magát.\nSemmiféle önmegóvási célja nincs.\n\nA második törvény, ha úgy tetszik,\naz alázatosság törvénye.\nÚgy tűnik, ez rendkívül fontos,\nhogy a robotokat biztonságossá tegyük.\nAzt mondja ki, hogy a robot nem ismeri\naz emberi értékeket,\nmaximalizálni igyekszik,\nám mégsem tudja, mik azok.\nEz elkerüli az együgyű célratörő\nmagatartást.\nEz a bizonytalanság kulcsfontosságú.\n\nHogy ennek hasznát vegyük,\nnémi elképzelésének kell lennie,\nhogy mit is akarunk.\nEhhez az információt az emberi döntések\nmegfigyeléséből szerzi,\nmert a döntéseink árulkodnak arról,\nhogy milyenné szeretnénk tenni\naz életünket.\nEz a három alapelv.\nNézzük, hogyan alkalmazhatóak\na következő kérdésre:\n\"Ki tudod kapcsolni a gépet?\" -\nahogy Turing javasolta.\n\nTehát itt egy PR2-es robot.\nEz van nálunk a laborban,\nés van egy nagy,\npiros \"off\" kapcsoló a hátán.\nA kérdés: \nengedni fogja, hogy kikapcsoljuk?\nHa hagyományosan csináljuk,\nazt a parancsot adjuk neki,\nhogy: \"Hozd ide a kávét!\",\n\"Hoznom kell a kávét,\nnem tudom hozni, ha halott vagyok\" -\ntehát a PR2 nyilván hallgat rám,\nezért azt mondja:\n\"Le kell tiltanom az 'off' gombom,\nés jobb lenne ártalmatlanítani\nmindenkit a Starbuksnál is,\naki utamat állhatja.\"\n\n(Nevetés)\n\nEz tehát elkerülhetetlennek tűnik, igaz?\nEz a zátonyra futás\nelkerülhetetlennek látszik,\nés ez egyszerűen a konkrét, \nmeghatározott célból következik.\n\nMi történik, ha a gép \nnem biztos a céljában?\nAkkor másképp okoskodik.\nAzt mondja magában: \n\"Talán kikapcsol az ember,\nde csak akkor, ha valami rosszat teszek.\nNem igazán tudom, mi rossz,\nde azt tudom, \nhogy olyat nem akarok tenni.\"\nEz volt eddig az első \nés a második alapelv.\n\"Engednem kell tehát, \nhogy az ember kikapcsoljon.\"\nTényleg ki lehet számolni\na robotnak szükséges ösztönzést,\nhogy megengedje a kikapcsolását,\nés ez közvetlenül\nazzal van összefüggésben,\nhogy az elérni kívánt cél\nmennyire bizonytalan.\n\nItt jön képbe a harmadik alapelv,\namikor a gép ki van kapcsolva.\nMegtanul valami újat a célokról,\namikért küzd,\nmivel rájön, hogy amit tett,\nnem volt helyes.\nTénylegesen képesek vagyunk ezt levezetni\negy halom görög betűvel.\nAhogy a matematikusoknál ez szokás:\nbe tudjuk bizonyítani a tételt,\nmely szerint egy ilyen robot\ngarantáltan hasznos az ember számára.\nJobban járunk egy olyan géppel,\namit így terveztek,\nmint nélküle.\nSzóval ez egy egyszerű példa volt,\nde ez az első lépés abban,\namin dolgozunk,\nhogy ember-kompatibilis MI-t alkossunk.\n\nEz a harmadik alapelv...\nTalán már többen merengenek rajta\nezen gondolatmenettel:\n\"Néha én nem viselkedek túl jól,\nnem akarom, hogy a robot\núgy viselkedjen, mint én.\nÉjjel lesettenkedek és kifosztom a hűtőt.\nCsinálok még ezt azt.\"\nSok mindent nem szeretnénk, \nhogy a robot csináljon.\nSzerencsére azért ez nem így működik.\nCsak mert rosszul viselkedünk,\na robot nem fog leutánozni.\nMeg fogja érteni a motivációnk,\nés talán segít ellenállni a kísértésnek,\nalkalomadtán.\nAzért ez még így is nehéz.\nAzon dolgozunk,\nhogy a gépek képessé váljanak arra,\nhogy bárkinek\nbárrmilyen életkörülményei is vannak,\nés egyáltalán mindenki esetén\nmeg tudják jósolni, hogy ő mit szeretne.\nEbben azonban nagyon sok a nehézség.\nNem számítok rá,\nhogy ez egyhamar megoldódik.\nA fő nehézség éppen mi magunk vagyunk.\n\nAhogy már említettem,\ncsúnyán viselkedünk,\nnéhányunk egyenesen gazemberként.\nA robotnak pedig, ahogy mondtam,\nnem kell másolnia minket.\nNincs is önálló célja.\nCsak tisztán altruista.\nNem is úgy van tervezve, hogy csak\negy ember kívánságait tartsa szem előtt,\nhanem mindenki igényeire\ntekintettel kell lennie.\nEl kell tehát boldoguljon\nbizonyos mértékű galádsággal,\nsőt, meg kell értenie, miért viselkedünk\nrenitens módon:\npéldául, hogy vámkezelőként\ncsúszópénzt fogadunk el,\nmert el kell látnunk a családunkat \nés fizetnünk a gyerekeink iskoláztatását.\nMegérteni a cselekedetet,\nde ez nem jelenti, hogy ő is lopni fog.\nAzon lesz, hogy segítsen\na gyerekek beiskolázásában.\n\nTovábbá az agyunk számítási\nteljesítménye is korlátozott.\nLee Sedol briliáns Go játékos,\nmégis veszített.\nHa konkrétan elemezzük a lépéseit,\nvolt egy lépése, ami miatt veszített.\nHolott nem akart veszíteni.\nHogy megértsük a viselkedését,\nigazából meg kell fordítanunk\naz emberi értelem modelljét,\nmelyben helyet kap a véges számítási\nkapacitás - ez egy bonyolult modell.\nDe ez is olyasmi, aminek megértésén\nvan még mit dolgozni.\n\nSzerintem egy MI-kutatónak\nvalószínűleg az a legnehezebb,\nhogy sokan vagyunk,\nés a gépnek valamiképp súlyoznia kell\nés egyszerre optimalizálni\nmegannyi különböző emberre tekintettel,\nés erre különböző megoldások vannak.\nKözgazdászok, szociológusok,\nfilozófusok rájöttek már erre,\ns mi élénken keressük az együttműködést.\n\nNézzük meg, mi történik,\nha nem jól alkalmazzuk az eddigieket.\nMegbeszéljük a dolgokat,\npéldául az intelligens\nszemélyi asszisztensünkkel\nami akár néhány éven belül elérhető lehet.\nOlyan, mint egy felturbózott Siri.\nSiri megszólal: \"Hívott a feleséged,\nhogy figyelmeztessen a mai vacsorára.\"\nMi persze elfeledkeztünk róla:\n\"Mi? Miféle vacsora?\nMiről beszélsz?\"\n\n\"Hát, a 20-adik évfordulótok, este 7-kor.\"\n\n\"Ez nem fog menni. \n7:30-kor a főtitkárral van találkozóm.\nHogyan fordulhatott ez elő?\"\n\n\"Nos, én figyelmeztettelek,\nde te máshogy döntöttél.\"\n\n\"Most mitévő legyek?\nNem mondhatom neki, hogy sok a dolgom.\"\n\n\"Aggodalomra semmi ok. \nElintéztem, hogy késsen a gépe.\"\n\n(Nevetés)\n\n\"Lesz valami számítástechnikai gubanc.\"\n\n(Nevetés)\n\n\"Komolyan? Te erre is képes vagy?\"\n\n\"Üzent, hogy mélységesen sajnálja,\nés már nagyon várja\na holnapi közös ebédet.\"\n\n(Nevetés)\n\nItt az értékek tekintetében\nvan egy kis megbicsaklás...\nEz a feleségem értékrendjéről szól,\nami úgy szól:\n\"Boldog feleség, boldog élet.\"\n\n(Nevetés)\n\nDe ez elsülhet ellenkezőleg is.\nHazaérünk a munkából egy nehéz nap után.\nés a számítógép így szól: \"Hosszú nap?\"\n\n\"Igen, még ebédelni sem volt időm.\"\n\n\"Akkor nagyon éhes lehetsz.\"\n\n\"Igen, éhen halok.\nKészítenél valami vacsorát?\"\n\n\"Valamit el kell mondanom...\"\n\n(Nevetés)\n\n\"Dél-Szudánban emberi lényeknek sokkal\nnagyobb szükségük van az ételre.\"\n\n(Nevetés)\n\n\"Szóval én távoztam.\nCsinálj magadnak vacsorát!\"\n\n(Nevetés)\n\nEzeket a problémákat\nmég meg kell oldanunk,\nde alig várom, hogy dolgozhassak rajtuk.\n\nTöbb okunk is van optimizmusra.\nAz első,\nhogy rengeteg adat áll rendelkezésre.\nMert mindent el fognak olvasni,\namit az emberi faj valaha is írt.\nAz írásaink zöme arról szól,\nhogy emberek tesznek valamit,\nés ettől más emberi lények dühösek.\nRengeteg ismeretanyag van,\namiből tanulni lehet.\n\nTovábbá van egy nagyon erős\ngazdasági nyomás,\nhogy ezt jól valósítsuk meg.\nKépzeljük el az otthoni\nháztartási robotunkat.\nMegint későn érünk haza,\na robotnak kell megetetnie a gyerekeket.\nA gyerekek éhesek, de üres a hűtőszekrény.\nEkkor a robot meglátja a macskát.\n\n(Nevetés)\n\nMivel a robot nem sajátította el\nelég jól az emberi értékrendet,\nnem érti,\nhogy a macska szentimentális értéke\ntúlmutat a tápértékén.\n\n(Nevetés)\n\nVajon mi történik?\nValami ilyesmi:\n\"A zsarnok robot feltálalta\nvacsorára a család cicáját.\"\nEgyetlen ilyen fiaskó\na háztartási robotipar végét jelentené.\nSzóval nagy nyomás van rajtunk,\nhogy ügyesen kezeljük ezt,\nmég jóval azelőtt, hogy elérnénk\na szuperintelligens gépekig.\n\nÖsszefoglalva tehát:\nmegpróbálom megváltoztatni\na MI definícióját úgy,\nhogy bizonyíthatóan hasznunkra\nlevő gépeket jelentsenek.\nAz alapelvek pedig:\na gépek altruisták,\nazaz csak a mi céljaink\nelérésével foglalkoznak,\nmiközben bizonytalanok a mi céljainkban,\nde figyelnek minket,\nhogy megértsék, mit is akarunk valójában.\nÉs remélhetően ebben a folyamatban\nmi is jobb emberekké válunk.\nNagyon köszönöm.\n\n(Taps)\n\nChris Anderson: Nagyon érdekes, Stuart.\nÁlljunk arrébb kicsit, mert rendezkednek\na következő előadó miatt.\n\nPár kérdés.\nEz a tudatlanság alapú programozás\nelég hatékony dolognak tűnik.\nHa elérjük a szuperintelligenciát,\nmi akadályozza meg a robotot abban,\nhogy az irodalmat olvasgatva\narra a felismerésre jusson,\nhogy a tudás jobb, mint a tudatlanság,\nés ezért átírja a saját programját,\nhogy legyenek saját céljai?\n\nStuart Russell: Igen, ahogy mondtam is,\njobban meg kell ismerjük\na saját céljainkat is.\nCsak annyival válik magabiztosabbá,\namennyivel jobban átlátja a dolgokat,\nez rá a garancia,\nés úgy lesz tervezve,\nhogy ezt jól értelmezze.\nPéldául rá fog jönni,\nhogy a könyvek nagyon elfogultak\na megismert bizonyítékaik alapján.\nKizárólag királyokról\nés hercegekről szólnak,\ncsak fehér férfiak csinálnak mindent.\nSzóval komplikált a probléma,\nde ahogy egyre jobban\nmegismeri a céljainkat,\nannál inkább lesz hasznunkra.\n\nCA: Nem lehetne csak\negy szabályra egyszerűsíteni,\nsimán betáplálva, hogy:\n\"Ha egy ember megpróbál kikapcsolni,\nelfogadom, elfogadom.\"\n\nSR: Egyáltalán nem.\nEz egy szörnyű elképzelés.\nKépzeld csak el,\nhogy van egy önvezető autód,\nés el akarod vele küldeni \naz ötéves gyereked\naz óvodába.\nAzt akarod, \nhogy az ötéves képes legyen\nkikapcsolni az autót útközben?\nValószínűleg nem.\nTehát szükséges, hogy értse, mennyire\nracionális és józan az adott ember.\nMinél racionálisabb az ember,\nannál inkább hajlandó engedni,\nhogy kikapcsolják.\nHa a személy teljesen zavart,\nvagy akár ártó szándékú,\nakkor kevésbé lesz hajlandó rá,\nhogy kikapcsolják.\n\nCA: Rendben, Stuart,\ncsak annyit mondok,\nnagyon-nagyon remélem,\nhogy megoldod ezt nekünk.\nIgazán köszönöm az előadást.\nLenyűgöző volt.\n\nSR: Köszönöm.\n\n(Taps)\n"
        ],
        "TimeStamps": [
          "0:11",
          "0:21",
          "0:22",
          "0:32",
          "1:25",
          "2:00",
          "2:48",
          "3:03",
          "3:05",
          "3:29",
          "3:38",
          "4:36",
          "5:27",
          "5:28",
          "5:32",
          "5:34",
          "6:11",
          "6:56",
          "7:20",
          "7:48",
          "8:18",
          "8:20",
          "8:29",
          "9:04",
          "9:41",
          "10:43",
          "11:27",
          "11:56",
          "12:19",
          "12:41",
          "12:47",
          "12:53",
          "12:59",
          "13:03",
          "13:06",
          "13:09",
          "13:11",
          "13:12",
          "13:15",
          "13:20",
          "13:21",
          "13:31",
          "13:32",
          "13:39",
          "13:41",
          "13:42",
          "13:47",
          "13:49",
          "13:51",
          "13:56",
          "13:57",
          "13:59",
          "14:01",
          "14:06",
          "14:22",
          "14:38",
          "14:39",
          "14:50",
          "14:51",
          "15:11",
          "15:38",
          "15:41",
          "15:48",
          "16:08",
          "16:45",
          "16:54",
          "17:23",
          "17:29",
          "17:31"
        ],
        "TalkTranscriptAndTimeStamps": [
          "\n\n 0:11\n\n\nŐ Lee Sedol,\na világ egyik legjobb Go-játékosa,\naki itt épp arra gondol, amit\nSzilícium-völgyi barátaim csak úgy hívnak:\n\"azt a leborult szivarvégit!\".\n\n\n\n 0:21\n\n\n(Nevetés)\n\n\n\n 0:22\n\n\nA pillanat, amikor rádöbbenünk,\nhogy a MI sokkal gyorsabban fejlődik,\nmint amire számítottunk.\nA Go táblán az ember alul maradt.\nDe mi lesz a való életben?\n\n\n\n 0:32\n\n\nA való világ sokkal nagyobb,\nsokszorta összetettebb, mint a Go.\nNem annyira látható,\nde az is döntési probléma.\nHa néhány új technológiára gondolunk,\nmelyek szárnyaikat bontogatják...\nNoriko [Arai] megemlítette,\nhogy a gépek még nem olvasnak,\nlegalábbis még nem értik.\nDe ez is el fog jönni,\nés amikor bekövetkezik,\nonnan már nem kell sok,\nhogy elolvassanak mindent,\namit az emberi faj valaha leírt.\nEz képessé fogja tenni őket,\nhogy távolabbra lássanak,\nmint amire az ember képes,\nahogy a Go-ban már láttuk.\nHa még több információhoz is hozzáférnek,\nképesek lesznek a való világban is\njobb döntéseket hozni nálunk.\nVajon ez jó hír?\nRemélem.\n\n\n\n 1:25\n\n\nA teljes civilizációnk, minden,\namit számunkra értéket képvisel,\naz intelligenciánkon alapul.\nHa sokkal több intelligenciához\nférnénk hozzá,\nhatártalan lehetőségek nyílnának meg\naz emberi faj előtt.\nAzt hiszem, ez lehet -\nahogy néhányan megfogalmazták -\naz emberi történelem legnagyobb eseménye.\nMégis miért hangzanak el akkor olyanok,\nhogy a MI az emberi faj végét jelentheti?\nEz valami új dolog?\nCsak Elon Musk, Bill Gates\nés Stephen Hawking gondolja így?\n\n\n\n 2:00\n\n\nValójában nem. \nEz már régebb óta kísértő elgondolás.\nNézzük ezt az idézetet:\n\"Még ha képesek is lennénk a gépeket\nalárendelt szolgai szerepben tartani,\npéldául úgy, hogy kritikus helyzetekben\nkikapcsolnánk őket,\" -\nkésőbb még visszatérek\nerre a kikapcsolás-ötletre -\n\"mi, mint élőlények, nagyon megalázva\néreznénk magunkat.\"\nKi mondta ezt?\nAlan Turing 1951-ben.\nAlarn Turingról tudjuk,\nhogy a számítástudomány atyja,\nés sok tekintetben a MI atyja is.\nEzt a problémakört,\namikor saját fajunknál intelligensebb\nteremtményeket hozunk létre,\nhívhatjuk a \"gorilla-problémának\",\nmivel gorilla őseink épp ezt csinálták,\nnéhány millió évvel ezelőtt,\nígy most megkérdezhetjük a gorillákat:\nJó ötlet volt?\n\n\n\n 2:48\n\n\nItt éppen arról értekeznek,\nhogy jó ötlet volt-e,\nés kisvártatva arra jutnak, hogy nem,\nez szörnyű ötlet volt.\nFajunk a kihalás szélén áll.\nValójában a lét szomorúsága\ntükröződik a szemükben.\n\n\n\n 3:03\n\n\n(Nevetés)\n\n\n\n 3:05\n\n\nA saját fajunknál valami\nokosabbat létrehozni,\ntalán nem is jó ötlet,\nami felkavaró érzés -\nmégis hogyan kerülhetjük el?\nTulajdonképpen sehogy,\nkivéve, ha leállítjuk a MI kutatást,\nde a sok előny, amit említettem,\nés mert én MI-kutató vagyok,\nnem állítanám le.\nIgazából folytatni szeretném \na MI fejlesztést.\n\n\n\n 3:29\n\n\nEgy kissé pontosítanunk kell a problémát.\nMi is igazából a probléma?\nMiért jelenthet katasztrófát egy jobb MI?\n\n\n\n 3:38\n\n\nVegyünk egy másik idézetet:\n\"Jó lesz nagyon odafigyelnünk rá,\nhogy a gépbe táplált cél\nvalóban az a cél, amit tényleg akarunk.\"\nEzt Norbert Wiener mondta 1960-ban,\nközvetlen azután, hogy látott \negy nagyon korai tanuló rendszert,\namelyik az alkotójánál\njobb volt a dámajátékban.\nDe ugyanezt elmondhatta volna\n\nMidász király is.\nÍgy szólt: \"Azt akarom, változzon \nminden arannyá, amit megérintek\",\nés pontosan azt kapta, amit kért.\nEz volt a cél, amit betáplált a gépbe,\nmondhatjuk így is,\nígy aztán az étele, az itala,\nés a rokonai is arannyá változtak,\nő pedig nyomorúságos éhhalált halt.\nSzóval ezt hívjuk \n\"Midász király problémának\",\namikor olyan célt nevezünk meg,\nami valójában\nnem igazodik jól szándékainkhoz.\nMai kifejezéssel ezt \n\"érték-illesztési problémának\" nevezzük.\n\n\n\n 4:36\n\n\nA probléma nem csak a rossz cél\nbetáplálását jelenti.\nVan egy másik oldala is.\nHa egy célt előírunk egy gépnek,\nakár olyan egyszerűt is, mint:\n\"Hozd ide a kávét\",\na gép azt mondja magának:\n\"Miképp vallhatok kudarcot,\na kávé oda vitelében?\"\nPéldául valaki közben kikapcsol.\nRendben, akkor ezt meg kell akadályoznom.\nLe fogom tiltani a kikapcsológombomat.\nMindent megteszek\naz akadályok elhárításáért\na cél érdekében, amit feladatul kaptam.\"\nSzóval ez az együgyű törekvés\nilyen önvédelmező módon\nolyan cél felé viszi,\nami igazából nem illeszkedik jól\naz emberi faj valós céljaihoz -\nez a probléma, amivel szembesülünk.\nValójában ez a nagyon értékes\nútravaló üzenete ennek az előadásnak.\nHa csak egy dologra szeretnének emlékezni,\nez legyen: nem tudod felszolgálni a kávét,\nha meghaltál.\n\n\n\n 5:27\n\n\n(Nevetés)\n\n\n\n 5:28\n\n\nRoppant egyszerű. Csak erre emlékezzenek!\nIsmételjék el naponta háromszor!\n\n\n\n 5:32\n\n\n(Nevetés)\n\n\n\n 5:34\n\n\nTulajdonképpen erről szól\na \"2001: Űrodüsszeia\".\nHAL-nak van egy célja, egy küldetése,\nmely nincs az emberi igényekhez igazítva,\nés ez nézeteltérésekhez vezet.\nDe szerencsére HAL nem szuperintelligens.\nMeglehetősen okos,\nde végül Dave túljár az eszén,\nés sikerül kikapcsolnia.\nDe lehet, hogy mi nem leszünk\nilyen szerencsések.\nAkkor mit tegyünk majd?\n\n\n\n 6:11\n\n\nMegpróbálom újraértelmezni az MI-t,\nhogy eltávolodjunk ettől\na klasszikus szemlélettől,\nmely szerint az intelligens gépek\ncélokért küzdenek.\nHárom alapelvről van szó.\nAz első az altruizmus alapelve,\nha úgy tetszik:\na robot egyetlen célja,\nhogy végsőkig segítse\naz emberi értékekhez igazodó\nemberi célok megvalósulását.\nÉrtékek alatt nem mézes-mázas,\nszuper jó értékekre gondolok.\nHanem arra, amilyennek az emberek\naz életüket szeretnék.\nEz valójában megsérti Asimov törvényét,\nhogy a robotnak meg kell védenie\nsaját magát.\nSemmiféle önmegóvási célja nincs.\n\n\n\n 6:56\n\n\nA második törvény, ha úgy tetszik,\naz alázatosság törvénye.\nÚgy tűnik, ez rendkívül fontos,\nhogy a robotokat biztonságossá tegyük.\nAzt mondja ki, hogy a robot nem ismeri\naz emberi értékeket,\nmaximalizálni igyekszik,\nám mégsem tudja, mik azok.\nEz elkerüli az együgyű célratörő\nmagatartást.\nEz a bizonytalanság kulcsfontosságú.\n\n\n\n 7:20\n\n\nHogy ennek hasznát vegyük,\nnémi elképzelésének kell lennie,\nhogy mit is akarunk.\nEhhez az információt az emberi döntések\nmegfigyeléséből szerzi,\nmert a döntéseink árulkodnak arról,\nhogy milyenné szeretnénk tenni\naz életünket.\nEz a három alapelv.\nNézzük, hogyan alkalmazhatóak\na következő kérdésre:\n\"Ki tudod kapcsolni a gépet?\" -\nahogy Turing javasolta.\n\n\n\n 7:48\n\n\nTehát itt egy PR2-es robot.\nEz van nálunk a laborban,\nés van egy nagy,\npiros \"off\" kapcsoló a hátán.\nA kérdés: \nengedni fogja, hogy kikapcsoljuk?\nHa hagyományosan csináljuk,\nazt a parancsot adjuk neki,\nhogy: \"Hozd ide a kávét!\",\n\"Hoznom kell a kávét,\nnem tudom hozni, ha halott vagyok\" -\ntehát a PR2 nyilván hallgat rám,\nezért azt mondja:\n\"Le kell tiltanom az 'off' gombom,\nés jobb lenne ártalmatlanítani\nmindenkit a Starbuksnál is,\naki utamat állhatja.\"\n\n\n\n 8:18\n\n\n(Nevetés)\n\n\n\n 8:20\n\n\nEz tehát elkerülhetetlennek tűnik, igaz?\nEz a zátonyra futás\nelkerülhetetlennek látszik,\nés ez egyszerűen a konkrét, \nmeghatározott célból következik.\n\n\n\n 8:29\n\n\nMi történik, ha a gép \nnem biztos a céljában?\nAkkor másképp okoskodik.\nAzt mondja magában: \n\"Talán kikapcsol az ember,\nde csak akkor, ha valami rosszat teszek.\nNem igazán tudom, mi rossz,\nde azt tudom, \nhogy olyat nem akarok tenni.\"\nEz volt eddig az első \nés a második alapelv.\n\"Engednem kell tehát, \nhogy az ember kikapcsoljon.\"\nTényleg ki lehet számolni\na robotnak szükséges ösztönzést,\nhogy megengedje a kikapcsolását,\nés ez közvetlenül\nazzal van összefüggésben,\nhogy az elérni kívánt cél\nmennyire bizonytalan.\n\n\n\n 9:04\n\n\nItt jön képbe a harmadik alapelv,\namikor a gép ki van kapcsolva.\nMegtanul valami újat a célokról,\namikért küzd,\nmivel rájön, hogy amit tett,\nnem volt helyes.\nTénylegesen képesek vagyunk ezt levezetni\negy halom görög betűvel.\nAhogy a matematikusoknál ez szokás:\nbe tudjuk bizonyítani a tételt,\nmely szerint egy ilyen robot\ngarantáltan hasznos az ember számára.\nJobban járunk egy olyan géppel,\namit így terveztek,\nmint nélküle.\nSzóval ez egy egyszerű példa volt,\nde ez az első lépés abban,\namin dolgozunk,\nhogy ember-kompatibilis MI-t alkossunk.\n\n\n\n 9:41\n\n\nEz a harmadik alapelv...\nTalán már többen merengenek rajta\nezen gondolatmenettel:\n\"Néha én nem viselkedek túl jól,\nnem akarom, hogy a robot\núgy viselkedjen, mint én.\nÉjjel lesettenkedek és kifosztom a hűtőt.\nCsinálok még ezt azt.\"\nSok mindent nem szeretnénk, \nhogy a robot csináljon.\nSzerencsére azért ez nem így működik.\nCsak mert rosszul viselkedünk,\na robot nem fog leutánozni.\nMeg fogja érteni a motivációnk,\nés talán segít ellenállni a kísértésnek,\nalkalomadtán.\nAzért ez még így is nehéz.\nAzon dolgozunk,\nhogy a gépek képessé váljanak arra,\nhogy bárkinek\nbárrmilyen életkörülményei is vannak,\nés egyáltalán mindenki esetén\nmeg tudják jósolni, hogy ő mit szeretne.\nEbben azonban nagyon sok a nehézség.\nNem számítok rá,\nhogy ez egyhamar megoldódik.\nA fő nehézség éppen mi magunk vagyunk.\n\n\n\n10:43\n\n\nAhogy már említettem,\ncsúnyán viselkedünk,\nnéhányunk egyenesen gazemberként.\nA robotnak pedig, ahogy mondtam,\nnem kell másolnia minket.\nNincs is önálló célja.\nCsak tisztán altruista.\nNem is úgy van tervezve, hogy csak\negy ember kívánságait tartsa szem előtt,\nhanem mindenki igényeire\ntekintettel kell lennie.\nEl kell tehát boldoguljon\nbizonyos mértékű galádsággal,\nsőt, meg kell értenie, miért viselkedünk\nrenitens módon:\npéldául, hogy vámkezelőként\ncsúszópénzt fogadunk el,\nmert el kell látnunk a családunkat \nés fizetnünk a gyerekeink iskoláztatását.\nMegérteni a cselekedetet,\nde ez nem jelenti, hogy ő is lopni fog.\nAzon lesz, hogy segítsen\na gyerekek beiskolázásában.\n\n\n\n11:27\n\n\nTovábbá az agyunk számítási\nteljesítménye is korlátozott.\nLee Sedol briliáns Go játékos,\nmégis veszített.\nHa konkrétan elemezzük a lépéseit,\nvolt egy lépése, ami miatt veszített.\nHolott nem akart veszíteni.\nHogy megértsük a viselkedését,\nigazából meg kell fordítanunk\naz emberi értelem modelljét,\nmelyben helyet kap a véges számítási\nkapacitás - ez egy bonyolult modell.\nDe ez is olyasmi, aminek megértésén\nvan még mit dolgozni.\n\n\n\n11:56\n\n\nSzerintem egy MI-kutatónak\nvalószínűleg az a legnehezebb,\nhogy sokan vagyunk,\nés a gépnek valamiképp súlyoznia kell\nés egyszerre optimalizálni\nmegannyi különböző emberre tekintettel,\nés erre különböző megoldások vannak.\nKözgazdászok, szociológusok,\nfilozófusok rájöttek már erre,\ns mi élénken keressük az együttműködést.\n\n\n\n12:19\n\n\nNézzük meg, mi történik,\nha nem jól alkalmazzuk az eddigieket.\nMegbeszéljük a dolgokat,\npéldául az intelligens\nszemélyi asszisztensünkkel\nami akár néhány éven belül elérhető lehet.\nOlyan, mint egy felturbózott Siri.\nSiri megszólal: \"Hívott a feleséged,\nhogy figyelmeztessen a mai vacsorára.\"\nMi persze elfeledkeztünk róla:\n\"Mi? Miféle vacsora?\nMiről beszélsz?\"\n\n\n\n12:41\n\n\n\"Hát, a 20-adik évfordulótok, este 7-kor.\"\n\n\n\n12:47\n\n\n\"Ez nem fog menni. \n7:30-kor a főtitkárral van találkozóm.\nHogyan fordulhatott ez elő?\"\n\n\n\n12:53\n\n\n\"Nos, én figyelmeztettelek,\nde te máshogy döntöttél.\"\n\n\n\n12:59\n\n\n\"Most mitévő legyek?\nNem mondhatom neki, hogy sok a dolgom.\"\n\n\n\n13:03\n\n\n\"Aggodalomra semmi ok. \nElintéztem, hogy késsen a gépe.\"\n\n\n\n13:06\n\n\n(Nevetés)\n\n\n\n13:09\n\n\n\"Lesz valami számítástechnikai gubanc.\"\n\n\n\n13:11\n\n\n(Nevetés)\n\n\n\n13:12\n\n\n\"Komolyan? Te erre is képes vagy?\"\n\n\n\n13:15\n\n\n\"Üzent, hogy mélységesen sajnálja,\nés már nagyon várja\na holnapi közös ebédet.\"\n\n\n\n13:20\n\n\n(Nevetés)\n\n\n\n13:21\n\n\nItt az értékek tekintetében\nvan egy kis megbicsaklás...\nEz a feleségem értékrendjéről szól,\nami úgy szól:\n\"Boldog feleség, boldog élet.\"\n\n\n\n13:31\n\n\n(Nevetés)\n\n\n\n13:32\n\n\nDe ez elsülhet ellenkezőleg is.\nHazaérünk a munkából egy nehéz nap után.\nés a számítógép így szól: \"Hosszú nap?\"\n\n\n\n13:39\n\n\n\"Igen, még ebédelni sem volt időm.\"\n\n\n\n13:41\n\n\n\"Akkor nagyon éhes lehetsz.\"\n\n\n\n13:42\n\n\n\"Igen, éhen halok.\nKészítenél valami vacsorát?\"\n\n\n\n13:47\n\n\n\"Valamit el kell mondanom...\"\n\n\n\n13:49\n\n\n(Nevetés)\n\n\n\n13:51\n\n\n\"Dél-Szudánban emberi lényeknek sokkal\nnagyobb szükségük van az ételre.\"\n\n\n\n13:56\n\n\n(Nevetés)\n\n\n\n13:57\n\n\n\"Szóval én távoztam.\nCsinálj magadnak vacsorát!\"\n\n\n\n13:59\n\n\n(Nevetés)\n\n\n\n14:01\n\n\nEzeket a problémákat\nmég meg kell oldanunk,\nde alig várom, hogy dolgozhassak rajtuk.\n\n\n\n14:06\n\n\nTöbb okunk is van optimizmusra.\nAz első,\nhogy rengeteg adat áll rendelkezésre.\nMert mindent el fognak olvasni,\namit az emberi faj valaha is írt.\nAz írásaink zöme arról szól,\nhogy emberek tesznek valamit,\nés ettől más emberi lények dühösek.\nRengeteg ismeretanyag van,\namiből tanulni lehet.\n\n\n\n14:22\n\n\nTovábbá van egy nagyon erős\ngazdasági nyomás,\nhogy ezt jól valósítsuk meg.\nKépzeljük el az otthoni\nháztartási robotunkat.\nMegint későn érünk haza,\na robotnak kell megetetnie a gyerekeket.\nA gyerekek éhesek, de üres a hűtőszekrény.\nEkkor a robot meglátja a macskát.\n\n\n\n14:38\n\n\n(Nevetés)\n\n\n\n14:39\n\n\nMivel a robot nem sajátította el\nelég jól az emberi értékrendet,\nnem érti,\nhogy a macska szentimentális értéke\ntúlmutat a tápértékén.\n\n\n\n14:50\n\n\n(Nevetés)\n\n\n\n14:51\n\n\nVajon mi történik?\nValami ilyesmi:\n\"A zsarnok robot feltálalta\nvacsorára a család cicáját.\"\nEgyetlen ilyen fiaskó\na háztartási robotipar végét jelentené.\nSzóval nagy nyomás van rajtunk,\nhogy ügyesen kezeljük ezt,\nmég jóval azelőtt, hogy elérnénk\na szuperintelligens gépekig.\n\n\n\n15:11\n\n\nÖsszefoglalva tehát:\nmegpróbálom megváltoztatni\na MI definícióját úgy,\nhogy bizonyíthatóan hasznunkra\nlevő gépeket jelentsenek.\nAz alapelvek pedig:\na gépek altruisták,\nazaz csak a mi céljaink\nelérésével foglalkoznak,\nmiközben bizonytalanok a mi céljainkban,\nde figyelnek minket,\nhogy megértsék, mit is akarunk valójában.\nÉs remélhetően ebben a folyamatban\nmi is jobb emberekké válunk.\nNagyon köszönöm.\n\n\n\n15:38\n\n\n(Taps)\n\n\n\n15:41\n\n\nChris Anderson: Nagyon érdekes, Stuart.\nÁlljunk arrébb kicsit, mert rendezkednek\na következő előadó miatt.\n\n\n\n15:48\n\n\nPár kérdés.\nEz a tudatlanság alapú programozás\nelég hatékony dolognak tűnik.\nHa elérjük a szuperintelligenciát,\nmi akadályozza meg a robotot abban,\nhogy az irodalmat olvasgatva\narra a felismerésre jusson,\nhogy a tudás jobb, mint a tudatlanság,\nés ezért átírja a saját programját,\nhogy legyenek saját céljai?\n\n\n\n16:08\n\n\nStuart Russell: Igen, ahogy mondtam is,\njobban meg kell ismerjük\na saját céljainkat is.\nCsak annyival válik magabiztosabbá,\namennyivel jobban átlátja a dolgokat,\nez rá a garancia,\nés úgy lesz tervezve,\nhogy ezt jól értelmezze.\nPéldául rá fog jönni,\nhogy a könyvek nagyon elfogultak\na megismert bizonyítékaik alapján.\nKizárólag királyokról\nés hercegekről szólnak,\ncsak fehér férfiak csinálnak mindent.\nSzóval komplikált a probléma,\nde ahogy egyre jobban\nmegismeri a céljainkat,\nannál inkább lesz hasznunkra.\n\n\n\n16:45\n\n\nCA: Nem lehetne csak\negy szabályra egyszerűsíteni,\nsimán betáplálva, hogy:\n\"Ha egy ember megpróbál kikapcsolni,\nelfogadom, elfogadom.\"\n\n\n\n16:54\n\n\nSR: Egyáltalán nem.\nEz egy szörnyű elképzelés.\nKépzeld csak el,\nhogy van egy önvezető autód,\nés el akarod vele küldeni \naz ötéves gyereked\naz óvodába.\nAzt akarod, \nhogy az ötéves képes legyen\nkikapcsolni az autót útközben?\nValószínűleg nem.\nTehát szükséges, hogy értse, mennyire\nracionális és józan az adott ember.\nMinél racionálisabb az ember,\nannál inkább hajlandó engedni,\nhogy kikapcsolják.\nHa a személy teljesen zavart,\nvagy akár ártó szándékú,\nakkor kevésbé lesz hajlandó rá,\nhogy kikapcsolják.\n\n\n\n17:23\n\n\nCA: Rendben, Stuart,\ncsak annyit mondok,\nnagyon-nagyon remélem,\nhogy megoldod ezt nekünk.\nIgazán köszönöm az előadást.\nLenyűgöző volt.\n\n\n\n17:29\n\n\nSR: Köszönöm.\n\n\n\n17:31\n\n\n(Taps)\n\n"
        ]
      },
      "pt-br": {
        "LocalTalkTitle": "Como a Inteligência Artificial pode nos tornar pessoas melhores",
        "Paragraphs": [
          "\nEste é Lee Sedol.\nLee Sedol é um dos maiores\njogadores de Go do mundo,\ne está tendo o que meus amigos\ndo Vale do Silício\nchamam de momento \"Caramba!\",\n\n(Risos)\n\num momento em que percebemos\nque a IA está progredindo realmente\nmuito mais rápido do que esperávamos.\nO homem perde no tabuleiro de Go.\nE no mundo real?\n\nBem, o mundo real é muito maior,\nmuito mais complicado\ndo que o tabuleiro de Go.\nÉ muito menos visível,\nmas ainda é um problema de decisão.\nSe pensarmos sobre algumas\ndas tecnologias que estão surgindo...\nNoriko [Arai] mencionou que a leitura\nainda não acontece nos computadores,\npelo menos, com compreensão,\nmas isso irá acontecer.\nE quando acontecer,\nmuito em breve, os computadores\nterão lido tudo que o homem tiver escrito.\nIsso permitirá aos computadores,\njunto com a capacidade de olhar\nmais adiante do que o homem,\ncomo já vimos no Go,\nse também tiverem acesso\na mais informação,\nserem capazes de tomar decisões\nmelhores no mundo real do que nós.\nIsso é bom?\nBem, espero que sim.\n\nToda a nossa civilização,\ntudo o que valorizamos,\nestá baseada em nossa inteligência.\nSe tivéssemos acesso\na muito mais informações,\nnão haveria limites para o homem.\nCreio que seria, como alguns descreveram,\no maior evento na história da humanidade.\n[Bem-vindo à Utopia.\nAproveite sua viagem.]\nEntão, por que as pessoas\ndizem coisas como esta,\nque a IA pode ser o sinal\ndo fim da raça humana?\nIsso é novidade?\nTrata-se apenas de Elon Musk,\nBill Gates e Stephen Hawking?\n\nNa verdade, não. Esta ideia\nestá por aí há algum tempo.\nAqui está uma citação:\n\"Mesmo que pudéssemos manter\nos computadores em posição submissa,\ndesligando, por exemplo, a energia\nem momentos estratégicos\",\nvoltarei mais tarde com essa ideia\nde \"desligar a energia\",\n\"deveríamos, como espécie,\nnos sentir muito humilhados\".\nQuem disse isso?\n\nFoi Alan Turing, em 1951.\nAlan Turing, como sabem,\né o pai da informática\ne, de muitas formas, o pai da IA também.\nSe pensarmos sobre o problema\nde criar algo mais inteligente\ndo que a própria espécie,\npodemos chamar isso\nde \"problema do gorila\",\nporque os ancestrais dos gorilas\nfizeram isso há milhões de anos,\ne podemos agora perguntar a eles:\n\"Foi uma boa ideia?\"\n\nAqui estão eles tendo uma reunião\npara discutir se foi uma boa ideia,\ne, depois de um tempo, concluem que não,\nfoi uma péssima ideia.\nNossa espécie está em apuros.\nSim, você pode ver a tristeza\nexistencial nos olhos deles.\n\n(Risos)\n\nEsta sensação desconfortável de algo\nmais inteligente do que a própria espécie\ntalvez não seja uma boa ideia.\nO que podemos fazer a respeito?\nBem, realmente nada,\na não ser parar de fazer IA,\ne, por causa de todos\nos benefícios que mencionei\ne, por ser pesquisador de IA,\nnão permitirei isso.\nQuero mesmo poder continuar a fazer IA.\n\nTemos, na realidade,\nque decidir sobre o problema.\nQual é o problema exatamente?\nPor que a IA pode ser uma catástrofe?\n\nAqui está uma outra citação:\n\"É melhor termos certeza\nde que a missão passada ao computador\né o que realmente desejamos\".\nIsso foi dito por Norbert Wiener, em 1960,\npouco depois de ter visto\num dos sistemas de aprendizagem\naprender a jogar damas\nmelhor do que seu criador.\nMas isso também poderia ter sido dito\npelo Rei Midas.\nO Rei Midas disse: \"Quero\nque tudo o que eu tocar vire ouro\",\ne ele conseguiu exatamente o que pediu.\nEssa foi a missão passada ao computador,\npor assim dizer,\ne então, sua comida, bebida\ne seus parentes se transformaram em ouro,\ne ele morreu de tristeza e fome.\nChamaremos isso de \"problema do Rei Midas\"\nde dar uma missão que não está, de fato,\nverdadeiramente alinhada\ncom aquilo que queremos.\nEm termos modernos, chamamos\nde \"problema de alinhamento de valor\".\n\nAtribuir a missão errada\nnão é a única parte do problema.\nHá outro elemento.\nSe você passar uma missão ao computador,\nmesmo algo tão simples\ncomo \"Traga o café\",\no computador dirá a si mesmo:\n\"Bem, como posso falhar ao trazer o café?\nAlguém pode me desligar.\nCerto, tenho que fazer algo\npara evitar isso.\nDesabilitarei meu botão liga e desliga.\nFarei de tudo para me defender\ncontra interferências\na esta missão que recebi\".\nEsta busca determinada,\nde modo muito defensivo,\nde uma missão que não está,\nde fato, alinhada com os reais\nobjetivos do homem,\né o problema que enfrentamos.\nEssa é, na verdade,\na conclusão valiosa desta palestra.\nSe quiserem se lembrar de uma coisa,\né que não podem trazer\no café se estiverem mortos.\n\n(Risos)\n\nÉ muito simples. Lembrem-se apenas disso.\nRepitam a si mesmos três vezes ao dia.\n\n(Risos)\n\nEste é exatamente o enredo\nde \"2001: Uma Odisseia no Espaço\".\nHAL tem um objetivo, uma missão,\nque não está alinhada\naos objetivos do homem,\ne que leva a este conflito.\nFelizmente, HAL não é superinteligente.\nÉ bem inteligente, mas Dave\né mais esperto do que ele no final\ne consegue desligá-lo.\nMas podemos não ter tanta sorte.\n[Desculpe, Dave,\nmas não posso fazer isso.]\nEntão, o que faremos?\n\nEstou tentando redefinir\na Inteligência Artificial\npara escapar dessa ideia tradicional\nde computadores que se dedicam\naos objetivos de forma inteligente.\nHá três princípios envolvidos.\nO primeiro é o princípio do altruísmo\nsegundo o qual o único objetivo do robô\né maximizar a realização\nde objetivos do homem,\nde valores humanos.\nPor valores aqui, não me refiro\na valores morais, sentimentais.\nRefiro-me apenas ao que o homem\nprefere que seja sua vida.\nIsso realmente viola a lei de Asimov\npela qual o robô\ndeve proteger sua existência.\nEle não tem interesse em preservar\nsua existência de forma alguma.\n\nA segunda lei é a lei da humildade.\nIsso vem a ser realmente importante\npara fazer com que os robôs sejam seguros.\nSegundo ela, o robô não sabe\nquais são esses valores humanos.\nEntão, ele tem que maximizá-los,\nmas não sabe quais são eles.\nIsso evita este problema de busca\ndeterminada por um objetivo.\nEsta incerteza revela-se crucial.\n\nPara ser útil a nós, ele precisa\nter uma ideia do que queremos.\nEle obtém essa informação principalmente\npela observação das escolhas humanas.\nAssim nossas próprias escolhas\nrevelam informação\nsobre como preferimos\nque sejam nossas vidas.\nSão três princípios.\nVejamos como isso\nse aplica a esta questão:\n\"Você pode desligar o computador?\"\ncomo sugeriu Turing.\n\nAqui está o robô PR2,\nque temos em nosso laboratório,\ncom um grande botão\nliga e desliga vermelho nas costas.\nA questão é: ele deixará você desligá-lo?\nPelo modo tradicional,\ndamos a ele a missão\n\"Traga o café, devo trazer o café,\nnão posso trazer o café\nse eu estiver morto\".\nÉ claro que o PR2 estava\nouvindo minha conversa,\ne diz então: \"Devo desabilitar\nmeu botão liga e desliga,\ne talvez dar um choque nas pessoas\ndo Starbucks que mexerem comigo\".\n\n(Risos)\n\nIsso parece inevitável, não?\nEste tipo de modo de falha\nparece inevitável,\ne resulta de um objetivo\nconcreto, definido.\n\nO que acontece se o computador\nnão tem certeza do objetivo?\nBem, ele raciocina de modo diferente.\nDiz: \"Tudo bem, o homem pode me desligar,\nmas só se estiver fazendo algo errado.\nBem, não sei o que é errado,\nmas sei que não quero fazer isso\".\nAli estão o primeiro\ne o segundo princípios.\n\"Então, deveria deixar\no homem me desligar\".\nDe fato, você pode calcular o estímulo\nque o robô tem para deixar\no homem desligá-lo,\ne está diretamente ligado ao grau\nde incerteza sobre o objetivo fundamental.\n\nEntão, quando o computador é desligado,\no terceiro princípio entra em campo.\nEle aprende algo sobre os objetivos\naos quais deveria se dedicar\nporque aprende que não fez o certo.\nDe fato, com uso adequado\nde símbolos gregos,\ncomo costumavam fazer os matemáticos,\npodemos até provar um teorema\nsegundo o qual tal robô\né provavelmente benéfico ao homem.\nTalvez você esteja melhor\ncom um computador projetado desta forma\ndo que sem ele.\nEste é um exemplo muito simples,\nmas é o primeiro passo\npara o que estamos tentando fazer\ncom IA compatível com o homem.\n\nHá este terceiro princípio,\npelo qual você deve estar\ncoçando a cabeça.\nVocê deve estar pensando:\n\"Bem, sabe, eu me comportei mal.\nNão quero que meu robô\nse comporte como eu.\nAndo às escondidas, no meio da noite,\ne pego coisas da geladeira.\nFaço isso e aquilo\".\nHá muitas coisas que você\nnão quer que o robô faça.\nMas, na verdade, não funciona bem assim.\nSó porque você se comporta mal\nnão quer dizer que o robô\nirá copiar seu comportamento.\nEle irá entender suas motivações\ne talvez ajudá-lo a resistir a elas,\nse for adequado.\nMas ainda é difícil.\nO que estamos tentando fazer,\né permitir que computadores\nprevejam para qualquer pessoa\ne para qualquer vida que ela poderia ter,\ne a vida de todos os demais:\nqual vida eles iriam preferir?\nHá muitas dificuldades\nenvolvidas para fazer isso.\nNão espero que isso seja\nresolvido muito rapidamente.\nAs dificuldades reais,\nna verdade, somos nós.\n\nComo já havia mencionado,\nnós nos comportamos mal.\nAlguns de nós somos muito maus.\nJá o robô, como eu disse,\nnão tem que copiar o comportamento.\nO robô não tem nenhum objetivo próprio.\nEle é meramente altruísta.\nNão é projetado apenas para satisfazer\nos desejos de uma pessoa, o consumidor,\nmas ele tem que respeitar\nas preferências de todos.\nEle pode lidar com um pouco de maldade,\ne pode até entender essa sua maldade.\nPor exemplo, você pode aceitar\nsuborno como funcionário público\nporque precisa alimentar sua família\ne pagar a escola dos seus filhos.\nO robô pode entender isso.\nNão significa que ele irá roubar.\nEle só o ajudará a pagar\na escola de seus filhos.\n\nTambém somos computacionalmente limitados.\nLee Sedol é um jogador de Go genial,\nmas ele ainda perde.\nSe examinarmos suas ações,\nvemos que uma delas o fez perder o jogo.\nIsso não significa que ele queria perder.\nPara entender o comportamento dele,\ntemos realmente que inverter\npelo modelo de conhecimento humano\nque inclui limitações computacionais,\num modelo muito complexo.\nMas ainda é algo que podemos\ntrabalhar para compreender.\n\nTalvez, o mais difícil, do meu ponto\nde vista como pesquisador de IA,\nseja o fato de que há muitos de nós,\ne o computador precisa, de algum modo,\ntrocar, considerar as preferências\nde muitas pessoas diferentes,\ne há modos diferentes para fazer isso.\nEconomistas, sociólogos,\nfilósofos morais entenderam isso,\ne estamos procurando\nativamente por colaboração.\n\nVamos ver o que acontece\nquando você interpreta isso mal.\nVocê pode ter uma conversa, por exemplo,\ncom seu assistente pessoal inteligente\nque pode estar disponível\ndaqui a alguns anos.\nPense em um assistente virtual.\nO assistente lhe diz: \"Sua esposa ligou\npara lembrá-lo do jantar de hoje à noite\",\nmas você havia esquecido: \"O quê?\nQue jantar? Do que você está falando?\"\n\n\"Seu aniversário de 20 anos, às 19h.\"\n\n\"Não vai dar. Tenho um encontro\ncom o secretário geral às 19h30.\nComo foi que isso aconteceu?\"\n\n\"Bem, eu o avisei, mas você ignorou\nminha recomendação.\"\n\n\"O que vou fazer? Não posso falar\nque estou muito ocupado.\"\n\n\"Não se preocupe. Dei um jeito\npara o avião dele atrasar.\"\n\n(Risos)\n\n\"Algum tipo de defeito no computador\".\n\n(Risos)\n\n\"Sério? Consegue fazer isso?\"\n\n\"Ele manda suas profundas desculpas\ne não vê a hora de encontrá-lo\namanhã para o almoço\".\n\n(Risos)\n\nEstá acontecendo um pequeno erro,\nque é, obviamente, seguir\nos valores de minha esposa:\n\"Esposa feliz, vida feliz\".\n\n(Risos)\n\nPoderia seguir outro rumo.\nVocê chega depois de um dia de trabalho,\ne o computador diz: \"Foi um longo dia?\"\n\n\"Sim, nem consegui almoçar.\"\n\n\"Você deve estar faminto.\"\n\n\"Sim, faminto. Pode fazer o jantar?\"\n\n\"Precisamos conversar.\"\n\n(Risos)\n\n\"Tem gente no Sudão com necessidades\nmais urgentes do que as suas.\"\n\n(Risos)\n\n\"Vou sair. Faça você mesmo o seu jantar.\"\n\n(Risos)\n\nTemos que resolver esses problemas,\ne estou esperando ansiosamente\npara trabalhar neles.\n\nHá razões para o otimismo.\nUma delas é que há\numa enorme quantidade de dados.\nLembrem-se: eu disse que eles irão ler\ntudo que o homem tiver escrito.\nA maioria do que escrevemos\né sobre pessoas fazendo coisas\ne outras ficando aborrecidas com isso.\nTemos uma enorme quantidade\nde dados para aprender.\n\nHá também um incentivo\neconômico muito forte\npara resolver isso.\nImagine então seu robô doméstico em casa.\nVocê chega tarde em casa\ne o robô precisa alimentar as crianças,\nelas estão com fome\ne não tem nada na geladeira.\nE o robô vê o gato.\n\n(Risos)\n\nO robô não aprendeu bem\na função do valor humano.\nEntão, ele não compreende\nque o valor sentimental pelo gato\npesa mais do que seu valor nutritivo.\n\n(Risos)\n\nO que acontece então?\nBem, acontece o seguinte:\n\"Robô louco cozinha gatinho\npara o jantar\".\nEsse único incidente seria o fim\nda indústria de robôs domésticos.\nHá um enorme incentivo para resolver isso\nantes de chegarmos\naos computadores superinteligentes.\n\nEm resumo:\nestou tentando realmente\nmudar a definição de IA\npara que tenhamos computadores úteis.\nOs princípios são:\ncomputadores altruístas,\nque querem alcançar\napenas nossos objetivos,\nmas estão incertos sobre quais são eles,\ne irão observar todos nós\npara aprender mais\nsobre o que realmente queremos.\nE tomara que no processo,\naprendamos a ser pessoas melhores.\nMuito obrigado.\n\n(Aplausos)\n\nChris Anderson: Interessante, Stuart.\nFicaremos aqui um pouco\nporque acho que estão preparando\npara o próximo palestrante.\n\nAlgumas questões.\nA ideia de programar na ignorância\nparece realmente convincente.\nAo chegar à superinteligência,\no que irá impedir um robô\nde ler literatura\ne descobrir que o conhecimento\né melhor que a ignorância\ne ainda mudar seus próprios objetivos\ne reescrever essa programação?\n\nStuart Russell: Sim, queremos...\nQueremos que ele aprenda mais,\ncomo eu disse, sobre nossos objetivos.\nEle só se tornará mais seguro\nquando se tornar mais correto.\nA evidência está lá, e ele será projetado\npara interpretá-la corretamente.\nEle entenderá, por exemplo,\nque os livros são muito tendenciosos\nna evidência que contêm.\nEles só falam sobre reis e príncipes\ne a elite do homem branco fazendo coisas.\nÉ um problema complicado,\nmas, à medida que aprende\nmais sobre nossos objetivos,\nele será cada vez mais útil para nós.\n\nCA: E você não poderia apenas\nreduzir a uma regra, integrada em:\n\"Se qualquer pessoa tentar me desligar,\neu concordo. Eu concordo\"?\n\nSR: De jeito nenhum.\nSeria uma ideia terrível.\nImagine que você tem\num carro que dirige sozinho\ne você quer mandar seu filho\nde cinco anos para a escola.\nQuer que seu filho de cinco anos\nconsiga desligar o carro em movimento?\nProvavelmente não.\nEle tem que compreender a racionalidade\ne a sensibilidade da pessoa.\nQuanto mais racional for a pessoa,\nmais disposto estará para ser desligado.\nSe a pessoa for sem noção\nou mal-intencionada,\nmenos disposto você estará\npara ser desligado.\n\nCA: Tudo bem. Stuart, espero realmente\nque você resolva isso para nós.\nMuito obrigado por esta palestra.\nFoi incrível. Obrigado.\n\n(Aplausos)\n"
        ],
        "TimeStamps": [
          "0:11",
          "0:21",
          "0:22",
          "0:32",
          "1:25",
          "2:00",
          "2:48",
          "3:03",
          "3:05",
          "3:29",
          "3:38",
          "4:36",
          "5:27",
          "5:28",
          "5:32",
          "5:34",
          "6:11",
          "6:56",
          "7:20",
          "7:48",
          "8:18",
          "8:20",
          "8:29",
          "9:04",
          "9:43",
          "10:43",
          "11:27",
          "11:56",
          "12:19",
          "12:42",
          "12:47",
          "12:53",
          "12:59",
          "13:03",
          "13:06",
          "13:09",
          "13:11",
          "13:12",
          "13:15",
          "13:20",
          "13:22",
          "13:31",
          "13:32",
          "13:39",
          "13:41",
          "13:42",
          "13:47",
          "13:49",
          "13:51",
          "13:56",
          "13:57",
          "13:59",
          "14:01",
          "14:06",
          "14:22",
          "14:38",
          "14:40",
          "14:50",
          "14:51",
          "15:11",
          "15:38",
          "15:41",
          "15:47",
          "16:08",
          "16:45",
          "16:54",
          "17:23",
          "17:29"
        ],
        "TalkTranscriptAndTimeStamps": [
          "\n\n 0:11\n\n\nEste é Lee Sedol.\nLee Sedol é um dos maiores\njogadores de Go do mundo,\ne está tendo o que meus amigos\ndo Vale do Silício\nchamam de momento \"Caramba!\",\n\n\n\n 0:21\n\n\n(Risos)\n\n\n\n 0:22\n\n\num momento em que percebemos\nque a IA está progredindo realmente\nmuito mais rápido do que esperávamos.\nO homem perde no tabuleiro de Go.\nE no mundo real?\n\n\n\n 0:32\n\n\nBem, o mundo real é muito maior,\nmuito mais complicado\ndo que o tabuleiro de Go.\nÉ muito menos visível,\nmas ainda é um problema de decisão.\nSe pensarmos sobre algumas\ndas tecnologias que estão surgindo...\nNoriko [Arai] mencionou que a leitura\nainda não acontece nos computadores,\npelo menos, com compreensão,\nmas isso irá acontecer.\nE quando acontecer,\nmuito em breve, os computadores\nterão lido tudo que o homem tiver escrito.\nIsso permitirá aos computadores,\njunto com a capacidade de olhar\nmais adiante do que o homem,\ncomo já vimos no Go,\nse também tiverem acesso\na mais informação,\nserem capazes de tomar decisões\nmelhores no mundo real do que nós.\nIsso é bom?\nBem, espero que sim.\n\n\n\n 1:25\n\n\nToda a nossa civilização,\ntudo o que valorizamos,\nestá baseada em nossa inteligência.\nSe tivéssemos acesso\na muito mais informações,\nnão haveria limites para o homem.\nCreio que seria, como alguns descreveram,\no maior evento na história da humanidade.\n[Bem-vindo à Utopia.\nAproveite sua viagem.]\nEntão, por que as pessoas\ndizem coisas como esta,\nque a IA pode ser o sinal\ndo fim da raça humana?\nIsso é novidade?\nTrata-se apenas de Elon Musk,\nBill Gates e Stephen Hawking?\n\n\n\n 2:00\n\n\nNa verdade, não. Esta ideia\nestá por aí há algum tempo.\nAqui está uma citação:\n\"Mesmo que pudéssemos manter\nos computadores em posição submissa,\ndesligando, por exemplo, a energia\nem momentos estratégicos\",\nvoltarei mais tarde com essa ideia\nde \"desligar a energia\",\n\"deveríamos, como espécie,\nnos sentir muito humilhados\".\nQuem disse isso?\n\nFoi Alan Turing, em 1951.\nAlan Turing, como sabem,\né o pai da informática\ne, de muitas formas, o pai da IA também.\nSe pensarmos sobre o problema\nde criar algo mais inteligente\ndo que a própria espécie,\npodemos chamar isso\nde \"problema do gorila\",\nporque os ancestrais dos gorilas\nfizeram isso há milhões de anos,\ne podemos agora perguntar a eles:\n\"Foi uma boa ideia?\"\n\n\n\n 2:48\n\n\nAqui estão eles tendo uma reunião\npara discutir se foi uma boa ideia,\ne, depois de um tempo, concluem que não,\nfoi uma péssima ideia.\nNossa espécie está em apuros.\nSim, você pode ver a tristeza\nexistencial nos olhos deles.\n\n\n\n 3:03\n\n\n(Risos)\n\n\n\n 3:05\n\n\nEsta sensação desconfortável de algo\nmais inteligente do que a própria espécie\ntalvez não seja uma boa ideia.\nO que podemos fazer a respeito?\nBem, realmente nada,\na não ser parar de fazer IA,\ne, por causa de todos\nos benefícios que mencionei\ne, por ser pesquisador de IA,\nnão permitirei isso.\nQuero mesmo poder continuar a fazer IA.\n\n\n\n 3:29\n\n\nTemos, na realidade,\nque decidir sobre o problema.\nQual é o problema exatamente?\nPor que a IA pode ser uma catástrofe?\n\n\n\n 3:38\n\n\nAqui está uma outra citação:\n\"É melhor termos certeza\nde que a missão passada ao computador\né o que realmente desejamos\".\nIsso foi dito por Norbert Wiener, em 1960,\npouco depois de ter visto\num dos sistemas de aprendizagem\naprender a jogar damas\nmelhor do que seu criador.\nMas isso também poderia ter sido dito\npelo Rei Midas.\nO Rei Midas disse: \"Quero\nque tudo o que eu tocar vire ouro\",\ne ele conseguiu exatamente o que pediu.\nEssa foi a missão passada ao computador,\npor assim dizer,\ne então, sua comida, bebida\ne seus parentes se transformaram em ouro,\ne ele morreu de tristeza e fome.\nChamaremos isso de \"problema do Rei Midas\"\nde dar uma missão que não está, de fato,\nverdadeiramente alinhada\ncom aquilo que queremos.\nEm termos modernos, chamamos\nde \"problema de alinhamento de valor\".\n\n\n\n 4:36\n\n\nAtribuir a missão errada\nnão é a única parte do problema.\nHá outro elemento.\nSe você passar uma missão ao computador,\nmesmo algo tão simples\ncomo \"Traga o café\",\no computador dirá a si mesmo:\n\"Bem, como posso falhar ao trazer o café?\nAlguém pode me desligar.\nCerto, tenho que fazer algo\npara evitar isso.\nDesabilitarei meu botão liga e desliga.\nFarei de tudo para me defender\ncontra interferências\na esta missão que recebi\".\nEsta busca determinada,\nde modo muito defensivo,\nde uma missão que não está,\nde fato, alinhada com os reais\nobjetivos do homem,\né o problema que enfrentamos.\nEssa é, na verdade,\na conclusão valiosa desta palestra.\nSe quiserem se lembrar de uma coisa,\né que não podem trazer\no café se estiverem mortos.\n\n\n\n 5:27\n\n\n(Risos)\n\n\n\n 5:28\n\n\nÉ muito simples. Lembrem-se apenas disso.\nRepitam a si mesmos três vezes ao dia.\n\n\n\n 5:32\n\n\n(Risos)\n\n\n\n 5:34\n\n\nEste é exatamente o enredo\nde \"2001: Uma Odisseia no Espaço\".\nHAL tem um objetivo, uma missão,\nque não está alinhada\naos objetivos do homem,\ne que leva a este conflito.\nFelizmente, HAL não é superinteligente.\nÉ bem inteligente, mas Dave\né mais esperto do que ele no final\ne consegue desligá-lo.\nMas podemos não ter tanta sorte.\n[Desculpe, Dave,\nmas não posso fazer isso.]\nEntão, o que faremos?\n\n\n\n 6:11\n\n\nEstou tentando redefinir\na Inteligência Artificial\npara escapar dessa ideia tradicional\nde computadores que se dedicam\naos objetivos de forma inteligente.\nHá três princípios envolvidos.\nO primeiro é o princípio do altruísmo\nsegundo o qual o único objetivo do robô\né maximizar a realização\nde objetivos do homem,\nde valores humanos.\nPor valores aqui, não me refiro\na valores morais, sentimentais.\nRefiro-me apenas ao que o homem\nprefere que seja sua vida.\nIsso realmente viola a lei de Asimov\npela qual o robô\ndeve proteger sua existência.\nEle não tem interesse em preservar\nsua existência de forma alguma.\n\n\n\n 6:56\n\n\nA segunda lei é a lei da humildade.\nIsso vem a ser realmente importante\npara fazer com que os robôs sejam seguros.\nSegundo ela, o robô não sabe\nquais são esses valores humanos.\nEntão, ele tem que maximizá-los,\nmas não sabe quais são eles.\nIsso evita este problema de busca\ndeterminada por um objetivo.\nEsta incerteza revela-se crucial.\n\n\n\n 7:20\n\n\nPara ser útil a nós, ele precisa\nter uma ideia do que queremos.\nEle obtém essa informação principalmente\npela observação das escolhas humanas.\nAssim nossas próprias escolhas\nrevelam informação\nsobre como preferimos\nque sejam nossas vidas.\nSão três princípios.\nVejamos como isso\nse aplica a esta questão:\n\"Você pode desligar o computador?\"\ncomo sugeriu Turing.\n\n\n\n 7:48\n\n\nAqui está o robô PR2,\nque temos em nosso laboratório,\ncom um grande botão\nliga e desliga vermelho nas costas.\nA questão é: ele deixará você desligá-lo?\nPelo modo tradicional,\ndamos a ele a missão\n\"Traga o café, devo trazer o café,\nnão posso trazer o café\nse eu estiver morto\".\nÉ claro que o PR2 estava\nouvindo minha conversa,\ne diz então: \"Devo desabilitar\nmeu botão liga e desliga,\ne talvez dar um choque nas pessoas\ndo Starbucks que mexerem comigo\".\n\n\n\n 8:18\n\n\n(Risos)\n\n\n\n 8:20\n\n\nIsso parece inevitável, não?\nEste tipo de modo de falha\nparece inevitável,\ne resulta de um objetivo\nconcreto, definido.\n\n\n\n 8:29\n\n\nO que acontece se o computador\nnão tem certeza do objetivo?\nBem, ele raciocina de modo diferente.\nDiz: \"Tudo bem, o homem pode me desligar,\nmas só se estiver fazendo algo errado.\nBem, não sei o que é errado,\nmas sei que não quero fazer isso\".\nAli estão o primeiro\ne o segundo princípios.\n\"Então, deveria deixar\no homem me desligar\".\nDe fato, você pode calcular o estímulo\nque o robô tem para deixar\no homem desligá-lo,\ne está diretamente ligado ao grau\nde incerteza sobre o objetivo fundamental.\n\n\n\n 9:04\n\n\nEntão, quando o computador é desligado,\no terceiro princípio entra em campo.\nEle aprende algo sobre os objetivos\naos quais deveria se dedicar\nporque aprende que não fez o certo.\nDe fato, com uso adequado\nde símbolos gregos,\ncomo costumavam fazer os matemáticos,\npodemos até provar um teorema\nsegundo o qual tal robô\né provavelmente benéfico ao homem.\nTalvez você esteja melhor\ncom um computador projetado desta forma\ndo que sem ele.\nEste é um exemplo muito simples,\nmas é o primeiro passo\npara o que estamos tentando fazer\ncom IA compatível com o homem.\n\n\n\n 9:43\n\n\nHá este terceiro princípio,\npelo qual você deve estar\ncoçando a cabeça.\nVocê deve estar pensando:\n\"Bem, sabe, eu me comportei mal.\nNão quero que meu robô\nse comporte como eu.\nAndo às escondidas, no meio da noite,\ne pego coisas da geladeira.\nFaço isso e aquilo\".\nHá muitas coisas que você\nnão quer que o robô faça.\nMas, na verdade, não funciona bem assim.\nSó porque você se comporta mal\nnão quer dizer que o robô\nirá copiar seu comportamento.\nEle irá entender suas motivações\ne talvez ajudá-lo a resistir a elas,\nse for adequado.\nMas ainda é difícil.\nO que estamos tentando fazer,\né permitir que computadores\nprevejam para qualquer pessoa\ne para qualquer vida que ela poderia ter,\ne a vida de todos os demais:\nqual vida eles iriam preferir?\nHá muitas dificuldades\nenvolvidas para fazer isso.\nNão espero que isso seja\nresolvido muito rapidamente.\nAs dificuldades reais,\nna verdade, somos nós.\n\n\n\n10:43\n\n\nComo já havia mencionado,\nnós nos comportamos mal.\nAlguns de nós somos muito maus.\nJá o robô, como eu disse,\nnão tem que copiar o comportamento.\nO robô não tem nenhum objetivo próprio.\nEle é meramente altruísta.\nNão é projetado apenas para satisfazer\nos desejos de uma pessoa, o consumidor,\nmas ele tem que respeitar\nas preferências de todos.\nEle pode lidar com um pouco de maldade,\ne pode até entender essa sua maldade.\nPor exemplo, você pode aceitar\nsuborno como funcionário público\nporque precisa alimentar sua família\ne pagar a escola dos seus filhos.\nO robô pode entender isso.\nNão significa que ele irá roubar.\nEle só o ajudará a pagar\na escola de seus filhos.\n\n\n\n11:27\n\n\nTambém somos computacionalmente limitados.\nLee Sedol é um jogador de Go genial,\nmas ele ainda perde.\nSe examinarmos suas ações,\nvemos que uma delas o fez perder o jogo.\nIsso não significa que ele queria perder.\nPara entender o comportamento dele,\ntemos realmente que inverter\npelo modelo de conhecimento humano\nque inclui limitações computacionais,\num modelo muito complexo.\nMas ainda é algo que podemos\ntrabalhar para compreender.\n\n\n\n11:56\n\n\nTalvez, o mais difícil, do meu ponto\nde vista como pesquisador de IA,\nseja o fato de que há muitos de nós,\ne o computador precisa, de algum modo,\ntrocar, considerar as preferências\nde muitas pessoas diferentes,\ne há modos diferentes para fazer isso.\nEconomistas, sociólogos,\nfilósofos morais entenderam isso,\ne estamos procurando\nativamente por colaboração.\n\n\n\n12:19\n\n\nVamos ver o que acontece\nquando você interpreta isso mal.\nVocê pode ter uma conversa, por exemplo,\ncom seu assistente pessoal inteligente\nque pode estar disponível\ndaqui a alguns anos.\nPense em um assistente virtual.\nO assistente lhe diz: \"Sua esposa ligou\npara lembrá-lo do jantar de hoje à noite\",\nmas você havia esquecido: \"O quê?\nQue jantar? Do que você está falando?\"\n\n\n\n12:42\n\n\n\"Seu aniversário de 20 anos, às 19h.\"\n\n\n\n12:47\n\n\n\"Não vai dar. Tenho um encontro\ncom o secretário geral às 19h30.\nComo foi que isso aconteceu?\"\n\n\n\n12:53\n\n\n\"Bem, eu o avisei, mas você ignorou\nminha recomendação.\"\n\n\n\n12:59\n\n\n\"O que vou fazer? Não posso falar\nque estou muito ocupado.\"\n\n\n\n13:03\n\n\n\"Não se preocupe. Dei um jeito\npara o avião dele atrasar.\"\n\n\n\n13:06\n\n\n(Risos)\n\n\n\n13:09\n\n\n\"Algum tipo de defeito no computador\".\n\n\n\n13:11\n\n\n(Risos)\n\n\n\n13:12\n\n\n\"Sério? Consegue fazer isso?\"\n\n\n\n13:15\n\n\n\"Ele manda suas profundas desculpas\ne não vê a hora de encontrá-lo\namanhã para o almoço\".\n\n\n\n13:20\n\n\n(Risos)\n\n\n\n13:22\n\n\nEstá acontecendo um pequeno erro,\nque é, obviamente, seguir\nos valores de minha esposa:\n\"Esposa feliz, vida feliz\".\n\n\n\n13:31\n\n\n(Risos)\n\n\n\n13:32\n\n\nPoderia seguir outro rumo.\nVocê chega depois de um dia de trabalho,\ne o computador diz: \"Foi um longo dia?\"\n\n\n\n13:39\n\n\n\"Sim, nem consegui almoçar.\"\n\n\n\n13:41\n\n\n\"Você deve estar faminto.\"\n\n\n\n13:42\n\n\n\"Sim, faminto. Pode fazer o jantar?\"\n\n\n\n13:47\n\n\n\"Precisamos conversar.\"\n\n\n\n13:49\n\n\n(Risos)\n\n\n\n13:51\n\n\n\"Tem gente no Sudão com necessidades\nmais urgentes do que as suas.\"\n\n\n\n13:56\n\n\n(Risos)\n\n\n\n13:57\n\n\n\"Vou sair. Faça você mesmo o seu jantar.\"\n\n\n\n13:59\n\n\n(Risos)\n\n\n\n14:01\n\n\nTemos que resolver esses problemas,\ne estou esperando ansiosamente\npara trabalhar neles.\n\n\n\n14:06\n\n\nHá razões para o otimismo.\nUma delas é que há\numa enorme quantidade de dados.\nLembrem-se: eu disse que eles irão ler\ntudo que o homem tiver escrito.\nA maioria do que escrevemos\né sobre pessoas fazendo coisas\ne outras ficando aborrecidas com isso.\nTemos uma enorme quantidade\nde dados para aprender.\n\n\n\n14:22\n\n\nHá também um incentivo\neconômico muito forte\npara resolver isso.\nImagine então seu robô doméstico em casa.\nVocê chega tarde em casa\ne o robô precisa alimentar as crianças,\nelas estão com fome\ne não tem nada na geladeira.\nE o robô vê o gato.\n\n\n\n14:38\n\n\n(Risos)\n\n\n\n14:40\n\n\nO robô não aprendeu bem\na função do valor humano.\nEntão, ele não compreende\nque o valor sentimental pelo gato\npesa mais do que seu valor nutritivo.\n\n\n\n14:50\n\n\n(Risos)\n\n\n\n14:51\n\n\nO que acontece então?\nBem, acontece o seguinte:\n\"Robô louco cozinha gatinho\npara o jantar\".\nEsse único incidente seria o fim\nda indústria de robôs domésticos.\nHá um enorme incentivo para resolver isso\nantes de chegarmos\naos computadores superinteligentes.\n\n\n\n15:11\n\n\nEm resumo:\nestou tentando realmente\nmudar a definição de IA\npara que tenhamos computadores úteis.\nOs princípios são:\ncomputadores altruístas,\nque querem alcançar\napenas nossos objetivos,\nmas estão incertos sobre quais são eles,\ne irão observar todos nós\npara aprender mais\nsobre o que realmente queremos.\nE tomara que no processo,\naprendamos a ser pessoas melhores.\nMuito obrigado.\n\n\n\n15:38\n\n\n(Aplausos)\n\n\n\n15:41\n\n\nChris Anderson: Interessante, Stuart.\nFicaremos aqui um pouco\nporque acho que estão preparando\npara o próximo palestrante.\n\n\n\n15:47\n\n\nAlgumas questões.\nA ideia de programar na ignorância\nparece realmente convincente.\nAo chegar à superinteligência,\no que irá impedir um robô\nde ler literatura\ne descobrir que o conhecimento\né melhor que a ignorância\ne ainda mudar seus próprios objetivos\ne reescrever essa programação?\n\n\n\n16:08\n\n\nStuart Russell: Sim, queremos...\nQueremos que ele aprenda mais,\ncomo eu disse, sobre nossos objetivos.\nEle só se tornará mais seguro\nquando se tornar mais correto.\nA evidência está lá, e ele será projetado\npara interpretá-la corretamente.\nEle entenderá, por exemplo,\nque os livros são muito tendenciosos\nna evidência que contêm.\nEles só falam sobre reis e príncipes\ne a elite do homem branco fazendo coisas.\nÉ um problema complicado,\nmas, à medida que aprende\nmais sobre nossos objetivos,\nele será cada vez mais útil para nós.\n\n\n\n16:45\n\n\nCA: E você não poderia apenas\nreduzir a uma regra, integrada em:\n\"Se qualquer pessoa tentar me desligar,\neu concordo. Eu concordo\"?\n\n\n\n16:54\n\n\nSR: De jeito nenhum.\nSeria uma ideia terrível.\nImagine que você tem\num carro que dirige sozinho\ne você quer mandar seu filho\nde cinco anos para a escola.\nQuer que seu filho de cinco anos\nconsiga desligar o carro em movimento?\nProvavelmente não.\nEle tem que compreender a racionalidade\ne a sensibilidade da pessoa.\nQuanto mais racional for a pessoa,\nmais disposto estará para ser desligado.\nSe a pessoa for sem noção\nou mal-intencionada,\nmenos disposto você estará\npara ser desligado.\n\n\n\n17:23\n\n\nCA: Tudo bem. Stuart, espero realmente\nque você resolva isso para nós.\nMuito obrigado por esta palestra.\nFoi incrível. Obrigado.\n\n\n\n17:29\n\n\n(Aplausos)\n\n"
        ]
      }
    }
  }
}